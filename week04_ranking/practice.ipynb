{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems YSDA Course\n",
    "## Practice №3 Ranking and diversity\n",
    "\n",
    "Similarly to practice №3, in this notebook we are going to implement recommender system components - late-stage rankers and diversity control modules (or rerankers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRanker, CatBoostClassifier, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grocery.recommender.primitives import Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL_VALUE = -999999999.0\n",
    "DATA_DIR = '../data/lavka'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\n",
    "f\"\"\"\n",
    "WITH\n",
    "    actions as (\n",
    "        SELECT\n",
    "        user_id,\n",
    "        source_type,\n",
    "        product_category,\n",
    "        product_id as item_id,\n",
    "        request_id,\n",
    "        action_type,\n",
    "        position_in_request,\n",
    "        make_timestamp(timestamp * 1000000) as timestamp,\n",
    "        date_trunc('day', make_timestamp(timestamp * 1000000)) as day,\n",
    "        row_number() over () as idx\n",
    "    FROM '{DATA_DIR}/train.parquet'\n",
    "    WHERE action_type IN ('AT_CartUpdate', 'AT_View', 'AT_Click')\n",
    "    ),\n",
    "    user_windows as (\n",
    "        SELECT\n",
    "            idx,\n",
    "            sum(case when action_type = 'AT_CartUpdate' then 1 else 0 end) over rolling_w as cart_updates,\n",
    "            sum(case when action_type = 'AT_View' then 1 else 0 end) over rolling_w as views,\n",
    "            sum(case when action_type = 'AT_Click' then 1 else 0 end) over rolling_w as clicks,\n",
    "            count(distinct (case when action_type = 'AT_CartUpdate' then request_id end)) over rolling_w as num_requests_with_cart_update,\n",
    "            count(distinct request_id) over rolling_w as total_requests,\n",
    "        FROM actions\n",
    "        WINDOW rolling_w as (\n",
    "            PARTITION BY \"user_id\"\n",
    "            ORDER BY \"day\" ASC\n",
    "            RANGE BETWEEN UNBOUNDED PRECEDING AND INTERVAL 1 DAYS PRECEDING\n",
    "        )\n",
    "    ),\n",
    "    user_features as (\n",
    "        SELECT\n",
    "            idx,\n",
    "            cart_updates / IF(views = 0, 1, views) as user_cart_update_turn_rate,\n",
    "            clicks / IF(views = 0, 1, views) as user_click_turn_rate,\n",
    "            num_requests_with_cart_update / IF(total_requests = 0, 1, total_requests) as user_conversion_rate,\n",
    "        FROM user_windows\n",
    "    ),\n",
    "    item_windows as (\n",
    "        SELECT\n",
    "            idx,\n",
    "            sum(case when action_type = 'AT_CartUpdate' then 1 else 0 end) over rolling_w as cart_updates,\n",
    "            sum(case when action_type = 'AT_View' then 1 else 0 end) over rolling_w as views,\n",
    "            sum(case when action_type = 'AT_Click' then 1 else 0 end) over rolling_w as clicks,\n",
    "            count(distinct (case when action_type = 'AT_CartUpdate' then request_id end)) over rolling_w as num_requests_with_cart_update,\n",
    "            count(distinct request_id) over rolling_w as total_requests,\n",
    "        FROM actions\n",
    "        WINDOW rolling_w as (\n",
    "            PARTITION BY \"item_id\"\n",
    "            ORDER BY \"day\" ASC\n",
    "            RANGE BETWEEN UNBOUNDED PRECEDING AND INTERVAL 1 DAYS PRECEDING\n",
    "        )\n",
    "    ),\n",
    "    item_features as (\n",
    "        SELECT\n",
    "            idx,\n",
    "            cart_updates / IF(views = 0, 1, views) as item_cart_update_turn_rate,\n",
    "            clicks / IF(views = 0, 1, views) as item_click_turn_rate,\n",
    "            num_requests_with_cart_update / IF(total_requests = 0, 1, total_requests) as item_conversion_rate,\n",
    "        FROM item_windows\n",
    "    ),\n",
    "    user2item_windows as (\n",
    "        SELECT\n",
    "            idx,\n",
    "            timestamp,\n",
    "            max(case when action_type = 'AT_CartUpdate' then timestamp end) over rolling_w as last_cart_update_timestamp,\n",
    "            sum(case when action_type = 'AT_CartUpdate' then 1 else 0 end) over rolling_w as cart_updates,\n",
    "            array_agg(timestamp) over rolling_w as cart_update_timestamps,\n",
    "        FROM actions\n",
    "        WINDOW rolling_w as (\n",
    "            PARTITION BY \"user_id\", \"item_id\"\n",
    "            ORDER BY \"day\" ASC\n",
    "            RANGE BETWEEN UNBOUNDED PRECEDING AND INTERVAL 1 DAYS PRECEDING\n",
    "        )\n",
    "    ),\n",
    "    user2item_features as (\n",
    "        SELECT\n",
    "            idx,\n",
    "            cart_updates as u2i_cart_updates,\n",
    "            datepart('day', timestamp - last_cart_update_timestamp) as u2i_days_since_last_cart_update,\n",
    "            list_avg(list_transform(list_zip(\n",
    "                array_pop_back(list_sort(cart_update_timestamps)),\n",
    "                array_pop_front(list_sort(cart_update_timestamps))\n",
    "            ), x -> datepart('day', x[2] - x[1]))) as u2i_mean_time_between_cartupdates\n",
    "        FROM user2item_windows\n",
    "    )\n",
    "    SELECT\n",
    "        a.*,\n",
    "        cast(a.source_type as string) as source_type,\n",
    "        cast(a.product_category as string) as product_category,\n",
    "        coalesce(if(isnan(uf.user_cart_update_turn_rate), null, uf.user_cart_update_turn_rate), {FILL_VALUE}) as user_cart_update_turn_rate,\n",
    "        coalesce(if(isnan(uf.user_click_turn_rate), null, uf.user_click_turn_rate), {FILL_VALUE}) as user_click_turn_rate,\n",
    "        coalesce(if(isnan(uf.user_conversion_rate), null, uf.user_conversion_rate), {FILL_VALUE}) as user_conversion_rate,\n",
    "        coalesce(if(isnan(if.item_cart_update_turn_rate), null, if.item_cart_update_turn_rate), {FILL_VALUE}) as item_cart_update_turn_rate,\n",
    "        coalesce(if(isnan(if.item_click_turn_rate), null, if.item_click_turn_rate), {FILL_VALUE}) as item_click_turn_rate,\n",
    "        coalesce(if(isnan(if.item_conversion_rate), null, if.item_conversion_rate), {FILL_VALUE}) as item_conversion_rate,\n",
    "        coalesce(if(isnan(u2if.u2i_cart_updates), null, u2if.u2i_cart_updates), {FILL_VALUE}) as u2i_cart_updates,\n",
    "        coalesce(if(isnan(u2if.u2i_mean_time_between_cartupdates), null, u2if.u2i_mean_time_between_cartupdates), {FILL_VALUE}) as u2i_mean_time_between_cartupdates,\n",
    "    FROM actions a\n",
    "    LEFT JOIN user_features uf\n",
    "    ON a.idx = uf.idx\n",
    "    LEFT JOIN item_features if\n",
    "    ON a.idx = if.idx\n",
    "    LEFT JOIN user2item_features u2if\n",
    "    ON a.idx = u2if.idx\n",
    "\"\"\"\n",
    ").to_parquet(f\"{DATA_DIR}/train_with_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pl.read_parquet(f\"{DATA_DIR}/train_with_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURE_COLUMNS = [\n",
    "    \"user_cart_update_turn_rate\",\n",
    "    \"user_click_turn_rate\",\n",
    "    \"user_conversion_rate\",\n",
    "    \"item_cart_update_turn_rate\",\n",
    "    \"item_click_turn_rate\",\n",
    "    \"item_conversion_rate\",\n",
    "    \"u2i_cart_updates\",\n",
    "    \"u2i_mean_time_between_cartupdates\",\n",
    "]\n",
    "\n",
    "CAT_FEATURE_COLUMNS = [\n",
    "    # \"source_type\",\n",
    "    \"product_category\",\n",
    "]\n",
    "\n",
    "FEATURE_COLUMNS = NUM_FEATURE_COLUMNS + CAT_FEATURE_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "conditions = functools.reduce(lambda a, b: a & b, [pl.col(feature) != FILL_VALUE for feature in NUM_FEATURE_COLUMNS])\n",
    "\n",
    "(\n",
    "    dataset\n",
    "    .filter(conditions)\n",
    "    .select(NUM_FEATURE_COLUMNS)\n",
    "    .describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_target(action_type: str):\n",
    "    if action_type == \"AT_CartUpdate\":\n",
    "        return 1.0\n",
    "    elif action_type == \"AT_View\":\n",
    "        return 0.0\n",
    "\n",
    "cbm_dataset = (\n",
    "    dataset\n",
    "    .sort(\"timestamp\")\n",
    "    .filter(pl.col(\"request_id\").is_not_null())\n",
    "    .filter(pl.col(\"action_type\").is_in([\"AT_CartUpdate\", \"AT_View\"]))\n",
    "    .with_columns(\n",
    "        pl.col(\"request_id\").cast(str).alias(\"group_id\"),\n",
    "        pl.col(\"action_type\").map_elements(build_target, return_dtype=float).alias(\"target\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        target=pl.col('target').max().over(partition_by=[pl.col('group_id'), pl.col('item_id')])\n",
    "    )\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "train, test = train_test_split(cbm_dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "train = train.sort(\"group_id\")\n",
    "test = test.sort(\"group_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(\n",
    "    train.select(FEATURE_COLUMNS).to_numpy(),\n",
    "    feature_names=FEATURE_COLUMNS,\n",
    "    cat_features=CAT_FEATURE_COLUMNS,\n",
    "    label=train[\"target\"].to_numpy(),\n",
    "    group_id=train[\"group_id\"].to_numpy()\n",
    ")\n",
    "test_pool = Pool(\n",
    "    test.select(FEATURE_COLUMNS).to_numpy(),\n",
    "    feature_names=FEATURE_COLUMNS,\n",
    "    cat_features=CAT_FEATURE_COLUMNS,\n",
    "    label=test[\"target\"].to_numpy(),\n",
    "    group_id=test[\"group_id\"].to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pool.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(iterations=100, eval_metric=\"NDCG:top=10\")\n",
    "model.fit(train_pool, eval_set=test_pool, early_stopping_rounds=20, plot=True, verbose=False)\n",
    "\n",
    "model.save_model(\"model.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame({\n",
    "    \"importance\": model.feature_importances_,\n",
    "    \"feature\": model.feature_names_\n",
    "}).sort(\"importance\", descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.eval_metrics(test_pool, [\"AUC:type=Ranking\", \"NDCG\", \"NDCG:top=10\"])\n",
    "for metric in metrics:\n",
    "    print(metric)\n",
    "    print(np.mean(metrics[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_diversity_at_k(test_dataset, k=10):\n",
    "    assert \"score\" in test_dataset.columns, \"test_dataset must contain 'score' column\"\n",
    "    assert \"group_id\" in test_dataset.columns, \"test_dataset must contain 'group_id' column\"\n",
    "    assert \"product_category\" in test_dataset.columns, \"test_dataset must contain 'product_category' column\"\n",
    "    return test_dataset.select(\n",
    "        (pl.col(\"product_category\")\n",
    "        .sort_by(\"score\", descending=True)\n",
    "        .head(k)\n",
    "        .n_unique() / k)\n",
    "        .over(\"group_id\", mapping_strategy=\"explode\")\n",
    "    ).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serendipity_at_k(test_dataset, train_dataset, k=10):\n",
    "    assert \"score\" in test_dataset.columns, \"test_dataset must contain 'score' column\"\n",
    "    assert \"target\" in test_dataset.columns, \"test_dataset must contain 'target' column\"\n",
    "    assert \"group_id\" in test_dataset.columns, \"test_dataset must contain 'group_id' column\"\n",
    "    history = (\n",
    "        train_dataset\n",
    "        .select(\n",
    "            pl.col(\"user_id\"),\n",
    "            pl.col(\"item_id\"),\n",
    "            pl.lit(True).alias(\"is_historic\"),\n",
    "        )\n",
    "        .unique()\n",
    "    )\n",
    "    test_dataset = (\n",
    "        test_dataset\n",
    "        .join(history, on=[\"user_id\", \"item_id\"])\n",
    "        .with_columns(\n",
    "            is_historic=pl.col(\"is_historic\").fill_null(False)\n",
    "        )\n",
    "        .with_columns(\n",
    "            is_serendipitous=(pl.col(\"is_historic\") & (pl.col(\"target\") > 0))\n",
    "        )\n",
    "    )\n",
    "    return test_dataset.select(\n",
    "        pl.col(\"is_serendipitous\")\n",
    "        .sort_by(\"score\", descending=True)\n",
    "        .head(k)\n",
    "        .mean()\n",
    "        .over(\"group_id\", mapping_strategy=\"explode\")\n",
    "    ).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Serendipity@5\")\n",
    "print(f\"{serendipity_at_k(test.with_columns(score=model.predict(test_pool)), train, k=5):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dataset\n",
    "    .group_by(\"request_id\")\n",
    "    .agg(\n",
    "        (pl.col(\"action_type\") == \"AT_CartUpdate\").sum().alias(\"cart_updates\"),\n",
    "        (pl.col(\"action_type\") == \"AT_View\").sum().alias(\"views\"),\n",
    "        (pl.col(\"action_type\") == \"AT_Click\").sum().alias(\"clicks\"),\n",
    "    )\n",
    "    .select(\n",
    "        pl.col(\"cart_updates\").mean().alias(\"cart_updates_mean\"),\n",
    "        pl.col(\"views\").mean().alias(\"views_mean\"),\n",
    "        pl.col(\"clicks\").mean().alias(\"clicks_mean\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_dataset = dataset.with_row_index()\n",
    "\n",
    "request_views = (\n",
    "    resampled_dataset\n",
    "    .filter(pl.col(\"action_type\") == \"AT_View\")\n",
    "    .select(\n",
    "        pl.col(\"index\").sort_by(\"position_in_request\").head(10).over(\"request_id\", mapping_strategy=\"explode\"),\n",
    "        pl.lit(True).alias(\"view_is_ok\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "request_clicks = (\n",
    "    resampled_dataset\n",
    "    .filter(pl.col(\"action_type\") == \"AT_Click\")\n",
    "    .select(\n",
    "        pl.col(\"index\").sort_by(\"position_in_request\").head(3).over(\"request_id\", mapping_strategy=\"explode\"),\n",
    "        pl.lit(True).alias(\"click_is_ok\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "requests_with_cartupdate = (\n",
    "    resampled_dataset\n",
    "    .filter(pl.col(\"action_type\") == \"AT_CartUpdate\")\n",
    "    .select(\"request_id\").unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_dataset = (\n",
    "    resampled_dataset\n",
    "    .filter(pl.col(\"request_id\").is_not_null())\n",
    "    .join(requests_with_cartupdate, on=\"request_id\", how=\"inner\")\n",
    "    .join(request_views, on=\"index\", how=\"left\")\n",
    "    .join(request_clicks, on=\"index\", how=\"left\")\n",
    "    .filter((pl.col(\"action_type\") == \"AT_CartUpdate\") | pl.col(\"view_is_ok\") | pl.col(\"click_is_ok\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_cbm_dataset = (\n",
    "    resampled_dataset\n",
    "    .filter(pl.col(\"action_type\").is_in([\"AT_CartUpdate\", \"AT_View\"]))\n",
    "    .with_columns(\n",
    "        pl.col(\"request_id\").cast(str).alias(\"group_id\"),\n",
    "        pl.col(\"action_type\").map_elements(build_target, return_dtype=float).alias(\"target\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        target=pl.col('target').max().over(partition_by=[pl.col('group_id'), pl.col('item_id')])\n",
    "    )\n",
    "    .unique()\n",
    "    .sort(\"timestamp\")\n",
    ")\n",
    "\n",
    "resampled_train, resampled_test = train_test_split(resampled_cbm_dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "resampled_train = resampled_train.sort(\"group_id\")\n",
    "resampled_test = resampled_test.sort(\"group_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_train_pool = Pool(\n",
    "    resampled_train.select(FEATURE_COLUMNS).to_numpy(),\n",
    "    feature_names=FEATURE_COLUMNS,\n",
    "    cat_features=CAT_FEATURE_COLUMNS,\n",
    "    label=resampled_train[\"target\"].to_numpy(),\n",
    "    group_id=resampled_train[\"group_id\"].to_numpy()\n",
    ")\n",
    "\n",
    "resampled_test_pool = Pool(\n",
    "    resampled_test.select(FEATURE_COLUMNS).to_numpy(),\n",
    "    feature_names=FEATURE_COLUMNS,\n",
    "    cat_features=CAT_FEATURE_COLUMNS,\n",
    "    label=resampled_test[\"target\"].to_numpy(),\n",
    "    group_id=resampled_test[\"group_id\"].to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(iterations=200, eval_metric=\"NDCG:top=10\")\n",
    "model.fit(resampled_train_pool, eval_set=resampled_test_pool, early_stopping_rounds=30, plot=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resampled requests\")\n",
    "metrics = model.eval_metrics(resampled_test_pool, [\"AUC:type=Ranking\", \"NDCG\", \"NDCG:top=10\"])\n",
    "for metric in metrics:\n",
    "    print(metric)\n",
    "    print(np.mean(metrics[metric]))\n",
    "\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Original requests\")\n",
    "metrics = model.eval_metrics(test_pool, [\"AUC:type=Ranking\", \"NDCG\", \"NDCG:top=10\"])\n",
    "for metric in metrics:\n",
    "    print(metric)\n",
    "    print(np.mean(metrics[metric]))\n",
    "print(\"Serendipity@5\")\n",
    "print(f\"{serendipity_at_k(test.with_columns(score=model.predict(test_pool)), train, k=5):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRanker(iterations=200, eval_metric=\"NDCG:top=10\")\n",
    "model.fit(resampled_train_pool, eval_set=resampled_test_pool, early_stopping_rounds=30, plot=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resampled requests\")\n",
    "metrics = model.eval_metrics(resampled_test_pool, [\"AUC:type=Ranking\", \"NDCG\", \"NDCG:top=10\"])\n",
    "for metric in metrics:\n",
    "    print(metric)\n",
    "    print(np.mean(metrics[metric]))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Original requests\")\n",
    "metrics = model.eval_metrics(test_pool, [\"AUC:type=Ranking\", \"NDCG\", \"NDCG:top=10\"])\n",
    "for metric in metrics:\n",
    "    print(metric)\n",
    "    print(np.mean(metrics[metric]))\n",
    "print(\"Serendipity@5\")\n",
    "print(f\"{serendipity_at_k(test.with_columns(score=model.predict(test_pool)), train, k=5):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-binary target for ranking loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_target(action_type: str):\n",
    "    if action_type == \"AT_CartUpdate\":\n",
    "        return 1.0\n",
    "    elif action_type == \"AT_Click\":\n",
    "        return 0.8\n",
    "    elif action_type == \"AT_View\":\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_cbm_dataset = (\n",
    "    resampled_dataset\n",
    "    .with_columns(\n",
    "        pl.col(\"request_id\").cast(str).alias(\"group_id\"),\n",
    "        pl.col(\"action_type\").map_elements(build_target, return_dtype=float).alias(\"target\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        target=pl.col('target').max().over(partition_by=[pl.col('group_id'), pl.col('item_id')])\n",
    "    )\n",
    "    .unique()\n",
    "    .sort(\"timestamp\")\n",
    ")\n",
    "\n",
    "resampled_train, resampled_test = train_test_split(resampled_cbm_dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "resampled_train = resampled_train.sort(\"group_id\")\n",
    "resampled_test = resampled_test.sort(\"group_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRanker(iterations=200, eval_metric=\"NDCG:top=10\")\n",
    "model.fit(resampled_train_pool, eval_set=resampled_test_pool, early_stopping_rounds=30, plot=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original requests\")\n",
    "metrics = model.eval_metrics(test_pool, [\"AUC:type=Ranking\", \"NDCG\", \"NDCG:top=10\"])\n",
    "for metric in metrics:\n",
    "    print(metric)\n",
    "    print(np.mean(metrics[metric]))\n",
    "print(\"Serendipity@5\")\n",
    "print(f\"{serendipity_at_k(test.with_columns(score=model.predict(test_pool)), train, k=5):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranker class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_day = '2023-11-02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\n",
    "f\"\"\"\n",
    "WITH\n",
    "    actions as (\n",
    "        SELECT\n",
    "            user_id,\n",
    "            product_id as item_id,\n",
    "            product_category,\n",
    "            request_id,\n",
    "            action_type,\n",
    "            position_in_request,\n",
    "            make_timestamp(timestamp * 1000000) as timestamp,\n",
    "            date_trunc('day', make_timestamp(timestamp * 1000000)) as day,\n",
    "        FROM '{DATA_DIR}/train.parquet'\n",
    "        WHERE action_type IN ('AT_CartUpdate', 'AT_View', 'AT_Click')\n",
    "        AND date_trunc('day', make_timestamp(timestamp * 1000000)) < date '{last_day}'\n",
    "    ),\n",
    "    user_windows as (\n",
    "        SELECT\n",
    "            user_id,\n",
    "            sum(case when action_type = 'AT_CartUpdate' then 1 else 0 end) as cart_updates,\n",
    "            sum(case when action_type = 'AT_View' then 1 else 0 end) as views,\n",
    "            sum(case when action_type = 'AT_Click' then 1 else 0 end) as clicks,\n",
    "            count(distinct (case when action_type = 'AT_CartUpdate' then request_id end)) as num_requests_with_cart_update,\n",
    "            count(distinct request_id) as total_requests,\n",
    "        FROM actions\n",
    "        GROUP BY user_id\n",
    "    ),\n",
    "    user_features as (\n",
    "        SELECT\n",
    "            user_id,\n",
    "            cart_updates / views as user_cart_update_turn_rate,\n",
    "            clicks / views as user_click_turn_rate,\n",
    "            num_requests_with_cart_update / total_requests as user_conversion_rate,\n",
    "        FROM user_windows\n",
    "    ),\n",
    "    item_windows as (\n",
    "        SELECT\n",
    "            item_id,\n",
    "            cast(max(product_category) as string) as product_category,\n",
    "            sum(case when action_type = 'AT_CartUpdate' then 1 else 0 end) as cart_updates,\n",
    "            sum(case when action_type = 'AT_View' then 1 else 0 end) as views,\n",
    "            sum(case when action_type = 'AT_Click' then 1 else 0 end) as clicks,\n",
    "            count(distinct (case when action_type = 'AT_CartUpdate' then request_id end)) as num_requests_with_cart_update,\n",
    "            count(distinct request_id) as total_requests,\n",
    "        FROM actions\n",
    "        GROUP BY item_id\n",
    "    ),\n",
    "    item_features as (\n",
    "        SELECT\n",
    "            item_id,\n",
    "            product_category,\n",
    "            cart_updates / views as item_cart_update_turn_rate,\n",
    "            clicks / views as item_click_turn_rate,\n",
    "            num_requests_with_cart_update / total_requests as item_conversion_rate,\n",
    "        FROM item_windows\n",
    "    ),\n",
    "    user2item_windows as (\n",
    "        SELECT\n",
    "            user_id,\n",
    "            item_id,\n",
    "            max(case when action_type = 'AT_CartUpdate' then timestamp end) as last_cart_update_timestamp,\n",
    "            sum(case when action_type = 'AT_CartUpdate' then 1 else 0 end) as cart_updates,\n",
    "            array_agg(timestamp) as cart_update_timestamps,\n",
    "        FROM actions\n",
    "        GROUP BY item_id, user_id\n",
    "    ),\n",
    "    user2item_features as (\n",
    "        SELECT\n",
    "            user_id,\n",
    "            item_id,\n",
    "            cart_updates as u2i_cart_updates,\n",
    "            list_avg(list_transform(list_zip(\n",
    "                array_pop_back(list_sort(cart_update_timestamps)),\n",
    "                array_pop_front(list_sort(cart_update_timestamps))\n",
    "            ), x -> datepart('day', x[2] - x[1]))) as u2i_mean_time_between_cartupdates\n",
    "        FROM user2item_windows\n",
    "    )\n",
    "    SELECT\n",
    "        u2if.user_id as user_id,\n",
    "        u2if.item_id as item_id,\n",
    "        if.product_category as product_category,\n",
    "        coalesce(if(isnan(uf.user_cart_update_turn_rate), null, uf.user_cart_update_turn_rate), -999999999.0) as user_cart_update_turn_rate,\n",
    "        coalesce(if(isnan(uf.user_click_turn_rate), null, uf.user_click_turn_rate), -999999999.0) as user_click_turn_rate,\n",
    "        coalesce(if(isnan(uf.user_conversion_rate), null, uf.user_conversion_rate), -999999999.0) as user_conversion_rate,\n",
    "        coalesce(if(isnan(if.item_cart_update_turn_rate), null, if.item_cart_update_turn_rate), -999999999.0) as item_cart_update_turn_rate,\n",
    "        coalesce(if(isnan(if.item_click_turn_rate), null, if.item_click_turn_rate), -999999999.0) as item_click_turn_rate,\n",
    "        coalesce(if(isnan(if.item_conversion_rate), null, if.item_conversion_rate), -999999999.0) as item_conversion_rate,\n",
    "        coalesce(if(isnan(u2if.u2i_cart_updates), null, u2if.u2i_cart_updates), -999999999.0) as u2i_cart_updates,\n",
    "        coalesce(if(isnan(u2if.u2i_mean_time_between_cartupdates), null, u2if.u2i_mean_time_between_cartupdates), -999999999.0) as u2i_mean_time_between_cartupdates\n",
    "    FROM user2item_features u2if\n",
    "    LEFT JOIN user_features uf\n",
    "    ON u2if.user_id = uf.user_id\n",
    "    FULL JOIN item_features if\n",
    "    ON u2if.item_id = if.item_id\n",
    "\"\"\").to_parquet(\"latest_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grocery.recommender.features import FeatureManager, StaticFeatureExtractor, FeatureStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature_names = [\"user_cart_update_turn_rate\", \"user_click_turn_rate\", \"user_conversion_rate\"]\n",
    "item_feature_names = [\"item_cart_update_turn_rate\", \"item_click_turn_rate\", \"item_conversion_rate\", \"product_category\"]\n",
    "user2item_feature_names = [\"u2i_cart_updates\", \"u2i_mean_time_between_cartupdates\"]\n",
    "\n",
    "features = pl.read_parquet(\"latest_features.parquet\")\n",
    "user_features = features.select(\"user_id\", *user_feature_names).unique()\n",
    "item_features = features.select(\"item_id\", *item_feature_names).unique()\n",
    "user2item_features = features.select(\"user_id\", \"item_id\", *user2item_feature_names).unique()\n",
    "\n",
    "user_storage = FeatureStorage()\n",
    "item_storage = FeatureStorage()\n",
    "user2item_storage = FeatureStorage()\n",
    "\n",
    "\n",
    "for feature_key in user_feature_names:\n",
    "    user_feature_dict = {\n",
    "        row[\"user_id\"]: row[feature_key]\n",
    "        for row in user_features\n",
    "        .select(\"user_id\", feature_key)\n",
    "        .to_dicts()\n",
    "    }\n",
    "    user_storage.add_feature(feature_key, user_feature_dict, -999999999.0)\n",
    "\n",
    "for feature_key in [\"item_cart_update_turn_rate\", \"item_click_turn_rate\", \"item_conversion_rate\"]:\n",
    "    item_feature_dict = {\n",
    "        row[\"item_id\"]: row[feature_key]\n",
    "        for row in item_features\n",
    "        .select(\"item_id\", feature_key)\n",
    "        .to_dicts()\n",
    "    }\n",
    "    item_storage.add_feature(feature_key, item_feature_dict, -999999999.0)\n",
    "    \n",
    "feature_key = \"product_category\"\n",
    "item_feature_dict = {\n",
    "    row[\"item_id\"]: row[feature_key]\n",
    "    for row in item_features\n",
    "    .select(\"item_id\", feature_key)\n",
    "    .to_dicts()\n",
    "}\n",
    "item_storage.add_feature(feature_key, item_feature_dict, \"EMPTY\")\n",
    "\n",
    "for feature_key in [\"u2i_cart_updates\", \"u2i_mean_time_between_cartupdates\"]:\n",
    "    user2item_feature_dict = {\n",
    "        (row[\"user_id\"], row[\"item_id\"]): row[feature_key]\n",
    "        for row in user2item_features\n",
    "        .select(\"user_id\", \"item_id\", feature_key)\n",
    "        .to_dicts()\n",
    "    }\n",
    "    user2item_storage.add_feature(feature_key, user2item_feature_dict, -999999999.0)\n",
    "\n",
    "def user_key(user_id, item_id):\n",
    "    return user_id\n",
    "\n",
    "def item_key(user_id, item_id):\n",
    "    return item_id\n",
    "\n",
    "def user_item_key(user_id, item_id):\n",
    "    return (user_id, item_id)\n",
    "\n",
    "manager = FeatureManager([\n",
    "    StaticFeatureExtractor(user_feature_names, user_storage, key=user_key),\n",
    "    StaticFeatureExtractor(item_feature_names, item_storage, key=item_key),\n",
    "    StaticFeatureExtractor(user2item_feature_names, user2item_storage, key=user_item_key),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import heapq\n",
    "\n",
    "\n",
    "class Ranker(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def rank(self, object_id: int, candidates: list[Candidate], n: int) -> list[Candidate]:\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def select_top_n(candidates: list[Candidate], feature: str, n: int, descending: bool = True):\n",
    "        if descending:\n",
    "            return heapq.nlargest(n, candidates, key=lambda x: x.features[feature])\n",
    "        else:\n",
    "            return heapq.nsmallest(n, candidates, key=lambda x: x.features[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import FeaturesData\n",
    "\n",
    "\n",
    "class GroceryCatboostRanker(Ranker):\n",
    "    def __init__(self,\n",
    "                 model_path: str,\n",
    "                 num_feature_schema: list[str],\n",
    "                 cat_feature_schema: list[str] | None = None,\n",
    "                 score_feature_name: str = \"cbm_relevance\"\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.model = CatBoostRanker()\n",
    "        self.model.load_model(fname=model_path)\n",
    "        self.num_feature_schema = num_feature_schema\n",
    "        self.cat_feature_schema = cat_feature_schema or []\n",
    "        self.score_feature_name = score_feature_name\n",
    "        self.fill_value = -9999999.0\n",
    "\n",
    "    def build_cbm_features(self, candidates: list[Candidate]) -> FeaturesData:\n",
    "        num_feature_array = np.array([\n",
    "            [candidate.features.get(feature, self.fill_value) for feature in self.num_feature_schema]\n",
    "            for candidate in candidates\n",
    "        ], dtype=np.float32)\n",
    "        cat_feature_array = np.array([\n",
    "            [candidate.features.get(feature, \"EMPTY\") for feature in self.cat_feature_schema]\n",
    "            for candidate in candidates\n",
    "        ], dtype=object)\n",
    "        return FeaturesData(\n",
    "            num_feature_data=num_feature_array,\n",
    "            cat_feature_data=cat_feature_array,\n",
    "            num_feature_names=self.num_feature_schema,\n",
    "            cat_feature_names=self.cat_feature_schema,\n",
    "        )\n",
    "\n",
    "    def rank(self, object_id: int, candidates: list[Candidate], n: int) -> list[Candidate]:\n",
    "        features = self.build_cbm_features(candidates)\n",
    "        scores = self.model.predict(features)\n",
    "        for candidate, score in zip(candidates, scores):\n",
    "            candidate.features[self.score_feature_name] = score\n",
    "        return self.select_top_n(candidates, self.score_feature_name, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = GroceryCatboostRanker(\n",
    "    model_path=\"model.cbm\",\n",
    "    num_feature_schema=NUM_FEATURE_COLUMNS,\n",
    "    cat_feature_schema=CAT_FEATURE_COLUMNS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_object_id = train[\"user_id\"].shuffle()[-1]\n",
    "sample_history = (\n",
    "    train\n",
    "    .filter(pl.col(\"user_id\") == sample_object_id)\n",
    "    [\"item_id\"]\n",
    "    .unique()\n",
    "    .to_list()\n",
    ")\n",
    "candidates = [Candidate(id=item_id) for item_id in sample_history]\n",
    "candidates = list(manager.extract(sample_object_id, candidates))\n",
    "ranked = ranker.rank(sample_object_id, candidates, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxSampler(Ranker):\n",
    "    def __init__(self,\n",
    "                 temperature: float = 0.1,\n",
    "                 relevance_feature_name: str = \"cbm_relevance\",\n",
    "                 sampled_rank_feature_name: str = \"sampled_cbm_relevance\",\n",
    "                 random_state: int | np.random.RandomState | None = 42\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.relevance_feature_name = relevance_feature_name\n",
    "        self.sampled_rank_feature_name = sampled_rank_feature_name\n",
    "        self.temperature = temperature\n",
    "        self.rng = np.random.default_rng(seed=random_state)\n",
    "    \n",
    "    def gumbel_max_trick(self, relevances: np.ndarray) -> np.ndarray:\n",
    "        noise = self.rng.gumbel(size=relevances.shape)\n",
    "        relevances = relevances + noise * self.temperature\n",
    "        return relevances\n",
    "\n",
    "    def rank(self, object_id: int, candidates: list[Candidate], n: int) -> list[Candidate]:\n",
    "        relevances = np.array([candidate.features[self.relevance_feature_name] for candidate in candidates])\n",
    "        probs = self.gumbel_max_trick(relevances)\n",
    "        for candidate, prob in zip(candidates, probs):\n",
    "            candidate.features[self.sampled_rank_feature_name] = prob\n",
    "        return self.select_top_n(candidates, self.sampled_rank_feature_name, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SoftmaxSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.rank(sample_object_id, ranked, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
