{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWT4fY6Hd7I3"
      },
      "source": [
        "# Семинар 5: Нейросетевое ранжирование\n",
        "### Семинарист: Матвеев Артем, Yandex\n",
        "\n",
        "В этом семинаре мы познакомимся с методами кодирование входных признаков для моделей нейросетевого ранжирвания: Mutisize Encoding with Unified Embeddings для категориальных признаков и Picewise Linear Encoding для вещественных. А также имплементируем DCNv2 и проверим всю схему на наборе данных Criteo Kaggle Dataset. Попробуем обогнать такой сильный бейзлайн на табличных данных как Catboost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "heGGt6crwhpA"
      },
      "outputs": [],
      "source": [
        "import typing as tp\n",
        "import polars as pl\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F8ykYo-eg9X"
      },
      "source": [
        "## 1. Downsampled Criteo Kaggle Dataset\n",
        "Данные лежат здесь: https://www.kaggle.com/datasets/dogrose/downsampled-criteo-kaggle-dataset/data.\n",
        "\n",
        "Criteo Dataset — это крупномасштабный набор данных для задач предсказания кликов (Click-Through Rate, CTR) в рекламе. Он используется для разработки и тестирования алгоритмов, прогнозирующих вероятность клика пользователя на рекламное объявление.\n",
        "\n",
        "**Особенности датасета:**\n",
        "- Содержит миллионы рекламных показов\n",
        "- Включает бинарную целевую переменную (клик/не клик)\n",
        "- 13 числовых признаков (I1-I13), представляющих различные счётчики\n",
        "- 26 категориальных признаков (C1-C26), анонимизированных и хэшированных\n",
        "- Данные разделены на обучающую (6 дней) и тестовую (1 день) выборки\n",
        "- Версия Downsampled Criteo Kaggle Dataset была уменьшена в 10 раз относительно оригинала.\n",
        "\n",
        "Оригинальный датасет содержит множество пропущенных значений и требует тщательной предобработки, которая реализована в предоставленном коде.\n",
        "\n",
        "Проводилось одноименное соревнование на Kaggle: https://www.kaggle.com/c/criteo-display-ad-challenge/overview. Наиболее успешные решения в соревнованиях Criteo на Kaggle обычно включали:\n",
        "\n",
        "**Методы предобработки данных:**\n",
        "- Логарифмическая трансформация числовых признаков (как реализовано в коде)\n",
        "- Count encoding или one-hot encoding для категориальных признаков\n",
        "- Feature hashing для категориальных признаков с высокой кардинальностью\n",
        "- Создание комбинированных признаков (feature interactions)\n",
        "\n",
        "**Алгоритмы:**\n",
        "- Градиентный бустинг: XGBoost, LightGBM, CatBoost\n",
        "- Глубокие нейронные сети, особенно Wide & Deep модели\n",
        "- Факторизационные машины (Field-aware Factorization Machines, FFM)\n",
        "- Ансамбли различных моделей\n",
        "\n",
        "Победители часто использовали комбинацию этих подходов, уделяя особое внимание инженерии признаков и обработке категориальных переменных с высокой кардинальностью."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_l8oSvuxFSP",
        "outputId": "d2bf50cd-5f55-401b-9512-677cdc04e1d5"
      },
      "outputs": [],
      "source": [
        "# !mkdir ./data\n",
        "# !curl -L -o ./data/downsampled-criteo-kaggle-dataset.zip\\\n",
        "#   https://www.kaggle.com/api/v1/datasets/download/dogrose/downsampled-criteo-kaggle-dataset\n",
        "# !unzip ./data/downsampled-criteo-kaggle-dataset.zip -d ./data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7b2Md-m-2C_u"
      },
      "outputs": [],
      "source": [
        "class CriteoDatasetUtils:\n",
        "    INT_COLS = [f'I{i + 1}' for i in range(13)]\n",
        "    CAT_COLS = [f'C{i + 1}' for i in range(26)]\n",
        "    LABEL_COl = 'label'\n",
        "\n",
        "    @classmethod\n",
        "    def preprocess_dense_features(cls, lf: pl.LazyFrame) -> pl.LazyFrame:\n",
        "        \"\"\"\n",
        "        Preprocess dense features:\n",
        "        - Fill missing values with 0\n",
        "        - Apply log transformation: log(x+1) or log(x+4)\n",
        "        \"\"\"\n",
        "        expressions = []\n",
        "        for col in cls.INT_COLS:\n",
        "            expressions.append(\n",
        "                pl.col(col).fill_null(0).add(1 if col != 'I2' else 4).log()\n",
        "            )\n",
        "        lf = lf.with_columns(expressions)\n",
        "        return lf\n",
        "\n",
        "    @classmethod\n",
        "    def preprocess_categorical_features(cls, lf: pl.LazyFrame) -> pl.LazyFrame:\n",
        "        \"\"\"\n",
        "        Preprocess categorical features:\n",
        "        - Fill missing values with zero string (\"00000000\")\n",
        "        - Convert from hex to Int64\n",
        "        \"\"\"\n",
        "        expressions = []\n",
        "        for col in cls.CAT_COLS:\n",
        "            expressions.append(\n",
        "                pl.col(col).fill_null(\"00000000\").str.to_integer(base=16)\n",
        "            )\n",
        "        lf = lf.with_columns(expressions)\n",
        "        return lf\n",
        "\n",
        "    @classmethod\n",
        "    def read_and_preprocess(cls, path: str) -> pl.DataFrame:\n",
        "        lf = pl.scan_parquet(path)\n",
        "        lf = cls.preprocess_categorical_features(lf)\n",
        "        lf = cls.preprocess_dense_features(lf)\n",
        "        return lf.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q0oZv1MNsHqh"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 40)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>label</th><th>I1</th><th>I2</th><th>I3</th><th>I4</th><th>I5</th><th>I6</th><th>I7</th><th>I8</th><th>I9</th><th>I10</th><th>I11</th><th>I12</th><th>I13</th><th>C1</th><th>C2</th><th>C3</th><th>C4</th><th>C5</th><th>C6</th><th>C7</th><th>C8</th><th>C9</th><th>C10</th><th>C11</th><th>C12</th><th>C13</th><th>C14</th><th>C15</th><th>C16</th><th>C17</th><th>C18</th><th>C19</th><th>C20</th><th>C21</th><th>C22</th><th>C23</th><th>C24</th><th>C25</th><th>C26</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>0.693147</td><td>1.609438</td><td>1.791759</td><td>0.0</td><td>7.23201</td><td>1.609438</td><td>2.772589</td><td>1.098612</td><td>5.204007</td><td>0.693147</td><td>1.098612</td><td>0.0</td><td>1.098612</td><td>1761418852</td><td>2162322587</td><td>4220739894</td><td>2068259780</td><td>633879704</td><td>2114768079</td><td>3732510136</td><td>529118562</td><td>2805916944</td><td>2832028932</td><td>2999688344</td><td>935969124</td><td>673490422</td><td>450684655</td><td>2343089050</td><td>2300273383</td><td>3854202482</td><td>4114618041</td><td>568184265</td><td>2972002973</td><td>129309004</td><td>0</td><td>974593739</td><td>3318023300</td><td>3904386055</td><td>2535972118</td></tr><tr><td>0</td><td>0.0</td><td>1.791759</td><td>6.45047</td><td>0.0</td><td>10.946781</td><td>0.0</td><td>0.0</td><td>1.791759</td><td>4.189655</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.098612</td><td>98275684</td><td>73979506</td><td>2062028047</td><td>2161661274</td><td>633879704</td><td>2114768079</td><td>69696505</td><td>185940084</td><td>2093428418</td><td>990438539</td><td>2116836373</td><td>3486017542</td><td>2443294225</td><td>2995026422</td><td>1478826667</td><td>342520061</td><td>2003624857</td><td>187896596</td><td>568184265</td><td>1480633834</td><td>3421256155</td><td>0</td><td>974593739</td><td>3470446969</td><td>3935970412</td><td>2589289724</td></tr><tr><td>1</td><td>0.0</td><td>3.931826</td><td>0.0</td><td>0.0</td><td>8.764053</td><td>3.663562</td><td>2.995732</td><td>2.397895</td><td>4.969813</td><td>0.0</td><td>2.397895</td><td>0.0</td><td>1.94591</td><td>342162125</td><td>950618017</td><td>574295574</td><td>3394569756</td><td>633879704</td><td>2114768079</td><td>1765007041</td><td>1530472565</td><td>2805916944</td><td>990438539</td><td>2248945707</td><td>359635439</td><td>812864628</td><td>450684655</td><td>242755870</td><td>1606371972</td><td>3854202482</td><td>429505257</td><td>0</td><td>0</td><td>3736690781</td><td>0</td><td>851920782</td><td>3829933919</td><td>0</td><td>0</td></tr><tr><td>1</td><td>0.0</td><td>5.638355</td><td>0.0</td><td>1.386294</td><td>8.898229</td><td>3.218876</td><td>1.94591</td><td>1.386294</td><td>4.59512</td><td>0.0</td><td>0.693147</td><td>0.0</td><td>1.386294</td><td>2364568165</td><td>2598325497</td><td>779549537</td><td>186309082</td><td>1291264903</td><td>4268462821</td><td>1977395914</td><td>185940084</td><td>2805916944</td><td>990438539</td><td>2326518504</td><td>2558959783</td><td>3989772238</td><td>131152527</td><td>716094755</td><td>864816393</td><td>3854202482</td><td>3353078997</td><td>0</td><td>0</td><td>1624430225</td><td>0</td><td>974593739</td><td>2427856751</td><td>0</td><td>0</td></tr><tr><td>0</td><td>0.0</td><td>3.73767</td><td>1.098612</td><td>1.609438</td><td>8.045588</td><td>5.010635</td><td>4.174387</td><td>3.89182</td><td>4.941642</td><td>0.0</td><td>1.94591</td><td>1.94591</td><td>1.609438</td><td>98275684</td><td>648577312</td><td>3492987491</td><td>3247169921</td><td>1291264903</td><td>4222442646</td><td>1062127239</td><td>529118562</td><td>2805916944</td><td>1919877373</td><td>3299735832</td><td>3753576955</td><td>2245779768</td><td>131152527</td><td>68076599</td><td>2223606570</td><td>2399067775</td><td>1465486885</td><td>0</td><td>0</td><td>1360682</td><td>0</td><td>851920782</td><td>991444060</td><td>0</td><td>0</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (5, 40)\n",
              "┌───────┬──────────┬──────────┬──────────┬───┬───────────┬────────────┬────────────┬────────────┐\n",
              "│ label ┆ I1       ┆ I2       ┆ I3       ┆ … ┆ C23       ┆ C24        ┆ C25        ┆ C26        │\n",
              "│ ---   ┆ ---      ┆ ---      ┆ ---      ┆   ┆ ---       ┆ ---        ┆ ---        ┆ ---        │\n",
              "│ i64   ┆ f64      ┆ f64      ┆ f64      ┆   ┆ i64       ┆ i64        ┆ i64        ┆ i64        │\n",
              "╞═══════╪══════════╪══════════╪══════════╪═══╪═══════════╪════════════╪════════════╪════════════╡\n",
              "│ 0     ┆ 0.693147 ┆ 1.609438 ┆ 1.791759 ┆ … ┆ 974593739 ┆ 3318023300 ┆ 3904386055 ┆ 2535972118 │\n",
              "│ 0     ┆ 0.0      ┆ 1.791759 ┆ 6.45047  ┆ … ┆ 974593739 ┆ 3470446969 ┆ 3935970412 ┆ 2589289724 │\n",
              "│ 1     ┆ 0.0      ┆ 3.931826 ┆ 0.0      ┆ … ┆ 851920782 ┆ 3829933919 ┆ 0          ┆ 0          │\n",
              "│ 1     ┆ 0.0      ┆ 5.638355 ┆ 0.0      ┆ … ┆ 974593739 ┆ 2427856751 ┆ 0          ┆ 0          │\n",
              "│ 0     ┆ 0.0      ┆ 3.73767  ┆ 1.098612 ┆ … ┆ 851920782 ┆ 991444060  ┆ 0          ┆ 0          │\n",
              "└───────┴──────────┴──────────┴──────────┴───┴───────────┴────────────┴────────────┴────────────┘"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DATASETS_PATH = '/content'\n",
        "DATASETS_PATH = './data'\n",
        "train_df = CriteoDatasetUtils.read_and_preprocess(f'{DATASETS_PATH}/criteo_train_6days_downsampled.parquet')\n",
        "test_df = CriteoDatasetUtils.read_and_preprocess(f'{DATASETS_PATH}/criteo_test_1day_downsampled.parquet')\n",
        "train_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert train_df.null_count().pipe(sum).item() == 0\n",
        "assert test_df.null_count().pipe(sum).item() == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Смотрим на количество уникальных значений категориальных признаков."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'C3': 1203747,\n",
              " 'C12': 1047892,\n",
              " 'C21': 925198,\n",
              " 'C16': 759151,\n",
              " 'C4': 378887,\n",
              " 'C24': 90428,\n",
              " 'C26': 59490,\n",
              " 'C10': 50127,\n",
              " 'C7': 11950,\n",
              " 'C15': 11775,\n",
              " 'C11': 5215,\n",
              " 'C18': 4756,\n",
              " 'C13': 3164,\n",
              " 'C19': 2036,\n",
              " 'C1': 1452,\n",
              " 'C8': 628,\n",
              " 'C2': 556,\n",
              " 'C5': 303,\n",
              " 'C25': 94,\n",
              " 'C14': 26,\n",
              " 'C6': 18,\n",
              " 'C22': 17,\n",
              " 'C23': 15,\n",
              " 'C17': 10,\n",
              " 'C20': 4,\n",
              " 'C9': 3}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unique_counts = {col: train_df[col].n_unique() for col in CriteoDatasetUtils.CAT_COLS}\n",
        "sorted_unique_counts = dict(\n",
        "    sorted(unique_counts.items(), key=lambda item: item[1], reverse=True)\n",
        ")\n",
        "sorted_unique_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Посчитаем необходимое количество GPU памяти."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17.38335418701172 GB\n"
          ]
        }
      ],
      "source": [
        "uniq_ids = sum(sorted_unique_counts.values())\n",
        "embedding_dim = 256\n",
        "bytes_in_float = 4\n",
        "mult = 4 # params + grads + moment1 + moment2\n",
        "bytes_in_gb = 1024 * 1024 * 1024 \n",
        "print(f'{uniq_ids * embedding_dim * bytes_in_float * mult / bytes_in_gb} GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SsXfo6pvhvz"
      },
      "source": [
        "## 2. Multisize Unified Embeddings\n",
        "\n",
        "Для кодирования категориальных признаков будем использовать Multisize Unified кодирование от Google DeepMind: [Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems](https://arxiv.org/abs/2305.12102).\n",
        "\n",
        "### Общая задача\n",
        "\n",
        "Дан $D = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_{|D|}, y_{|D|})\\}$ с примерами из $T$ категориальных признаков с словарями $\\{V_1, V_2, \\ldots, V_T\\}$. Каждый пример $x = [v_1, v_2, \\ldots, v_T]$, где $v_i \\in V_i$.\n",
        "\n",
        "- Матрица эмбеддингов $\\mathbf{E} \\in \\mathbb{R}^{M \\times d}$, отображения примера в эмбеддинг $g(\\mathbf{x}; \\mathbf{E})$. \n",
        "- Хеш-функция $h(v) : V \\rightarrow [M]$ назначает значение признака индексу строки (используется в $g(\\mathbf{x}; \\mathbf{E})$).\n",
        "- Функция модели $f(\\mathbf{e}; \\boldsymbol{\\theta})$ преобразует вложения в предсказание.\n",
        "\n",
        "Задача обучения:\n",
        "$$\\arg \\min_{\\mathbf{E}, \\boldsymbol{\\theta}} \\mathcal{L}_D(\\mathbf{E}, \\boldsymbol{\\theta}), \\quad \\text{где} \\quad \\mathcal{L}_D(\\mathbf{E}, \\boldsymbol{\\theta}) = \\sum_{(\\mathbf{x},y) \\in D} \\ell(f(g(\\mathbf{x}; \\mathbf{E}); \\boldsymbol{\\theta}), y).$$\n",
        "\n",
        "Используем $h_t(v)$ для каждого признака $t \\in [T]$. Обозначаем $\\mathbf{e}_m$ для $m$-й строки $\\mathbf{E}$, и $\\mathbb{1}_{u,v}$ как индикатор коллизии хешей между $u$ и $v$.\n",
        "\n",
        "### Как это работает\n",
        "\n",
        "Далее будем предполагать, что $|T|$ = 2.\n",
        "\n",
        "<div style=\"width:90%; margin: auto;\">\n",
        "\n",
        "![](https://i.ibb.co/GKMKcvm/unified-embeddings.png)\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Почему это работает (интуиция)\n",
        "\n",
        "Рассмотрим частный случай (решаем бинарную классфикацию с помощью логистической регрессии):\n",
        "\n",
        "$$y_i \\in \\{0, 1\\}$$\n",
        "$$ D_0 = \\{(x_i, y_i) \\in D : y_i = 0\\} $$\n",
        "$$ D_1 = \\{(x_i, y_i) \\in D : y_i = 1\\} $$\n",
        "$$ C_{u,v,0} = |\\{([u, v], y) \\in D : y = 0\\}|$$\n",
        "$$ \\sigma_\\theta(z) = \\frac{1}{1 + \\exp(-\\langle z, \\theta \\rangle)} $$\n",
        "$$ z = g(x; \\mathbf{E}) = [e_{h_1(x_1)}, e_{h_2(x_2)}] $$\n",
        "$$ \\theta = [\\theta_1, \\theta_2],~\\theta_t \\in \\mathbb{R}^M$$\n",
        "\n",
        "\n",
        "Функция потерь бинарной кросс-энтропии:\n",
        "$$ \\mathcal{L}_D(\\mathbf{E}, \\theta) = - \\sum_{(x,y)\\in D_0} \\log \\left( \\frac{1}{1 + \\exp(-\\langle \\theta, g(x; \\mathbf{E}) \\rangle)} \\right) - \\sum_{(x,y)\\in D_1} \\log \\left( \\frac{1}{1 + \\exp(\\langle \\theta, g(x; \\mathbf{E}) \\rangle)} \\right) $$\n",
        "\n",
        "Перепишем функция потерь (через частоты совместного появления):\n",
        "$$ e_{u,v} = [e_{h_1(u)}, e_{h_2(v)}] $$\n",
        "\n",
        "$$ \\mathcal{L}_D(\\mathbf{E}, \\theta) = - \\sum_{u\\in V_1} \\sum_{v\\in V_2} C_{u,v,0} \\log \\left( \\frac{1}{1 + \\exp(-\\theta^\\top e_{u,v})} \\right) + C_{u,v,1} \\log \\left( \\frac{1}{1 + \\exp(\\theta^\\top e_{u,v})} \\right) $$\n",
        "\n",
        "После объединения сигмоидных функций:\n",
        "$$ \\mathcal{L}_D(\\mathbf{E}, \\theta) = - \\sum_{u\\in V_1} \\sum_{v\\in V_2} C_{u,v,0} \\log \\exp(\\theta^\\top e_{u,v}) - (C_{u,v,0} + C_{u,v,1}) \\log(1 + \\exp(\\theta^\\top e_{u,v})) $$\n",
        "\n",
        "Далее будем предполагать, что обучаем наш алгоритм с SGD. Посчитаем градиенты по эмбеддингам. Полный градиент для эмбеддинга с учетом внутри- и межпризнаковых взаимодействий:\n",
        "$$ \\nabla_{E_{h(u)}} \\mathcal{L}_D(\\mathbf{E}, \\theta) = $$\n",
        "$$ \\theta_1 \\sum_{v\\in V_2} C_{u,v,0} - (C_{u,v,0} + C_{u,v,1})\\sigma_\\theta(e_{u,v}) \\tag{1}$$\n",
        "$$ + \\theta_1 \\sum_{w\\in V_1, w\\neq u} \\mathbb{1}_{u,w} \\sum_{v\\in V_2} C_{w,v,0} - (C_{w,v,0} + C_{w,v,1})\\sigma_\\theta(e_{u,v}) \\tag{2}$$\n",
        "$$ + \\theta_2 \\sum_{v\\in V_2} \\mathbb{1}_{u,v} \\sum_{w\\in V_1} C_{w,v,0} - (C_{w,v,0} + C_{w,v,1})\\sigma_\\theta(e_{w,u}) \\tag{3}$$\n",
        "\n",
        "Анализируем:\n",
        "- $(1)$ collisionless компонента.\n",
        "- $(2)$ intra-feature компонента.\n",
        "- $(3)$ inter-feature компонента.\n",
        "- Компоненты $(2)$ и $(3)$ смещают реальный градиент.\n",
        "- Intra-feature bias сонаправлен с collisionless компонентой, поэтому модель не может убрать это смещение.\n",
        "- В случае SGD inter-feature bias может невелирован за счет $\\theta_1$ ортогонально $\\theta_2$, т.к. во время SGD, $e_{h(u)}$ представляет собой линейную комбинацию градиентов по шагам обучения, что означает, что $e_{h(u)}$ может быть разложено на компоненты в направлении $\\theta_1$ и inter-feature компоненты в направлении $\\theta_2$. Поскольку $\\langle\\theta_1, \\theta_2\\rangle = 0$, проекция $\\theta_1^\\top e_{h(u)}$ эффективно устраняет inter-feature  компонент.\n",
        "\n",
        "<div style=\"width:70%; margin: auto;\">\n",
        "\n",
        "![](https://i.ibb.co/xt6nbrZ3/theory-unified.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "Вывод: \n",
        "\n",
        "Не все коллизии одинаково проблематичны. Межпризнаковые коллизии могут быть смягчены однослойной нейронной сетью, поскольку разные признаки обрабатываются разными параметрами модели. Коллизии в рамках одного признака убираем за счет нескольких взятий хешей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nRvHPMaN9vrI"
      },
      "outputs": [],
      "source": [
        "class MultihashTransform:\n",
        "    \"\"\"\n",
        "    Applys transformation to training sample\n",
        "    \"\"\"\n",
        "    def __init__(self, cardinality, seeds=None, name='sparse'):\n",
        "        assert seeds is not None\n",
        "        self._cardinality = cardinality\n",
        "        self._name = name\n",
        "        self._seeds = torch.tensor(seeds)\n",
        "\n",
        "    def __call__(self, sample: dict[str, tp.Any]) -> dict[str, tp.Any]:\n",
        "        sample[self._name] = (\n",
        "            (sample[self._name].unsqueeze(1) + self._seeds) % self._cardinality\n",
        "        ).long().reshape(-1)\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ApiYoIQZVPXw"
      },
      "outputs": [],
      "source": [
        "seeds = [\n",
        "    [2342 + 13 * i, 7777 + 17 * i]\n",
        "    for i in range(26)\n",
        "]\n",
        "transform = MultihashTransform(10, seeds)\n",
        "input = {\n",
        "    'label': torch.tensor(1),\n",
        "    'dense': torch.randn(13),\n",
        "    'sparse': torch.arange(26)\n",
        "}\n",
        "output = transform(input)\n",
        "assert output['sparse'].shape == (2 * 26,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2, 7, 6, 5, 0, 3, 4, 1, 8, 9, 2, 7, 6, 5, 0, 3, 4, 1, 8, 9, 2, 7, 6, 5,\n",
            "        0, 3, 4, 1, 8, 9, 2, 7, 6, 5, 0, 3, 4, 1, 8, 9, 2, 7, 6, 5, 0, 3, 4, 1,\n",
            "        8, 9, 2, 7])\n"
          ]
        }
      ],
      "source": [
        "print(output['sparse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LvGGvGaoV99w"
      },
      "outputs": [],
      "source": [
        "class UnifiedEmbeddings(nn.Module):\n",
        "    def __init__(self, cardinality, embedding_dim):\n",
        "        super().__init__()\n",
        "        self._cardinality = cardinality\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(\n",
        "            num_embeddings=cardinality, embedding_dim=embedding_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, ids: torch.tensor):\n",
        "        # ids shape: [batch_size, num_features]\n",
        "        return self.embeddings(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItOaKUJYvn4w"
      },
      "source": [
        "## 3. Picewise Linear Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Для кодирования вещественных признаков будем использовать Picewise Linear Encoding от Yandex Research: [On Embeddings for Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556).\n",
        "\n",
        "GitHub: https://github.com/yandex-research/rtdl-num-embeddings.\n",
        "\n",
        "<div style=\"width:90%; margin: auto;\">\n",
        "\n",
        "![](https://i.ibb.co/XZtk6fSN/picewise-linear.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "Эмбеддинги числовых признаков формализуются как $z_i = f_i(x_i^{(num)}) \\in \\mathbb{R}^{d_i}$, где:\n",
        "- $f_i(x)$ — функция эмбеддинга для i-го числового признака\n",
        "- $z_i$ — результирующий вектор эмбеддинга\n",
        "- $d_i$ — размерность эмбеддинга\n",
        "\n",
        "Ключевые особенности:\n",
        "- Эмбеддинги вычисляются независимо для каждого признака\n",
        "- В MLP-архитектурах эмбеддинги конкатенируются в один вектор\n",
        "- В Transformer-архитектурах эмбеддинги используются без дополнительных преобразований\n",
        "\n",
        "**Кусочно-линейное кодирование (PLE)**\n",
        "\n",
        "PLE разбивает диапазон значений числового признака на $T$ интервалов (бинов) $B_1, \\ldots, B_T$ с границами $[b_0, b_1], [b_1, b_2], ..., [b_{T-1}, b_T]$.\n",
        "\n",
        "Формальное определение: $\\text{PLE}(x) = [e_1, \\ldots, e_T] \\in \\mathbb{R}^T$\n",
        "\n",
        "где компоненты $e_t$ вычисляются как:\n",
        "\n",
        "$$\n",
        "e_t = \n",
        "\\begin{cases}\n",
        "0, & \\text{если } x < b_{t - 1} \\text{ И } t > 1 \\\\\n",
        "1, & \\text{если } x \\geq b_t \\text{ И } t < T \\\\\n",
        "\\frac{x-b_{t-1}}{b_t-b_{t-1}}, & \\text{иначе}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Важные свойства:\n",
        "\n",
        "- При $T = 1$ PLE эквивалентно скалярному представлению\n",
        "- В отличие от категориальных признаков, PLE учитывает упорядоченность числовых данных\n",
        "- PLE можно рассматривать как предобработку признаков\n",
        "\n",
        "Применение в моделях с вниманием:\n",
        "\n",
        "В моделях с механизмом внимания требуется дополнительно учитывать информацию об индексах признаков:\n",
        "\n",
        "1. Для каждого бина $B_t$ выделяется обучаемый эмбеддинг $v_t \\in \\mathbb{R}^d$\n",
        "2. Итоговый эмбеддинг вычисляется как: $f_i(x) = v_0 + \\sum_{t=1}^T e_t \\cdot v_t = \\text{Linear}(\\text{PLE}(x))$\n",
        "\n",
        "Построение бинов:\n",
        "\n",
        "Наиболее распространенный подход к построению бинов — разбиение по квантилям эмпирического распределения числового признака. Формально:\n",
        "- Для i-го признака: $b_t = q_t \\left(\\{x_j^{(num)}\\}_{j \\in J_{train}}\\right)$, где $q$ — функция эмпирического квантиля\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cJczvUQvfnI7"
      },
      "outputs": [],
      "source": [
        "class PiecewiseLinearEncodingTransform:\n",
        "    \"\"\"\n",
        "    Applys transformation to training sample\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def compute_bins(\n",
        "        X: torch.Tensor,\n",
        "        n_bins: int,\n",
        "    ) -> list[torch.Tensor]:\n",
        "        bins = [\n",
        "            q.unique()\n",
        "            for q in torch.quantile(\n",
        "                X, torch.linspace(0.0, 1.0, n_bins + 1).to(X), dim=0\n",
        "            ).T\n",
        "        ]\n",
        "        return bins\n",
        "\n",
        "    def __init__(self, dense_train_df, n_bins=32, name='dense'):\n",
        "        self._name = name\n",
        "        self._bins = PiecewiseLinearEncodingTransform.compute_bins(dense_train_df.to_torch(), n_bins)\n",
        "        n_features = len(self._bins)\n",
        "        self._n_bins = [len(x) - 1 for x in self._bins]\n",
        "        max_n_bins = max(self._n_bins)\n",
        "\n",
        "        self.weight = torch.zeros(n_features, max_n_bins)\n",
        "        self.bias = torch.zeros(n_features, max_n_bins)\n",
        "\n",
        "        for i, bin_edges in enumerate(self._bins):\n",
        "            bin_width = bin_edges.diff()\n",
        "            w = 1.0 / bin_width\n",
        "            b = -bin_edges[:-1] / bin_width\n",
        "            self.weight[i, -1] = w[-1]\n",
        "            self.bias[i, -1] = b[-1]\n",
        "            self.weight[i, :self._n_bins[i] - 1] = w[:-1]\n",
        "            self.bias[i, :self._n_bins[i] - 1] = b[:-1]\n",
        "    \n",
        "    @property\n",
        "    def n_bins(self):\n",
        "        return self._n_bins\n",
        "\n",
        "    def __call__(self, sample: dict[str, tp.Any]) -> dict[str, tp.Any]:\n",
        "        x = sample[self._name].to(torch.float32).unsqueeze(0)\n",
        "        x = torch.addcmul(self.bias, self.weight, x[..., None])\n",
        "        x = torch.cat(\n",
        "            [\n",
        "                x[..., :1].clamp_max(1.0),\n",
        "                x[..., 1:-1].clamp(0.0, 1.0),\n",
        "                x[..., -1:].clamp_min(0.0)\n",
        "            ],\n",
        "            dim=-1,\n",
        "        )\n",
        "        x = x.flatten(-2).squeeze(0)\n",
        "        sample[self._name] = x\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Пример:\n",
        "\n",
        "**weight** = \n",
        "\\begin{bmatrix}\n",
        "\\frac{1}{b_1 - b_0} & \\frac{1}{b_2 - b_1} & \\frac{1}{b_3 - b_2} & \\frac{1}{b_4 - b_3} \\\\\n",
        "\\frac{1}{c_1 - c_0} & \\frac{1}{c_2 - c_1} & \\frac{1}{c_3 - c_2} & \\frac{1}{c_4 - c_3}\n",
        "\\end{bmatrix}\n",
        "\n",
        "**bias** = \n",
        "\\begin{bmatrix}\n",
        "-\\frac{b_0}{b_1 - b_0} & -\\frac{b_1}{b_2 - b_1} & -\\frac{b_2}{b_3 - b_2} & -\\frac{b_3}{b_4 - b_3} \\\\\n",
        "-\\frac{c_0}{c_1 - c_0} & -\\frac{c_1}{c_2 - c_1} & -\\frac{c_2}{c_3 - c_2} & -\\frac{c_3}{c_4 - c_3}\n",
        "\\end{bmatrix}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = PiecewiseLinearEncodingTransform(train_df[CriteoDatasetUtils.INT_COLS], name='dense') \n",
        "input = {\n",
        "    'label': torch.tensor(1),\n",
        "    'dense': torch.randn(13),\n",
        "    'sparse': torch.arange(26)\n",
        "}\n",
        "output = transform(input)\n",
        "assert output['dense'].shape == (403,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "c6-fc_dxhx6R"
      },
      "outputs": [],
      "source": [
        "class PiecewiseLinearEncoding(nn.Identity):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-5y1zKpv0L3"
      },
      "source": [
        "## 4. DCN v2 - deep cross network\n",
        "\n",
        "Для агрегации категориальных и вещественных признаков в один скаляр будем использовать DCNv2 от Google DeepMind: [DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems](https://arxiv.org/abs/2008.13535)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Подход\n",
        "\n",
        "<div style=\"width:50%; margin: auto;\">\n",
        "\n",
        "![](https://i.ibb.co/ZqfF5yf/dcn-v2.png)\n",
        "![](https://i.ibb.co/SDYWNSMy/dcn-v2-equation.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "Stacked вариант:\n",
        "- Сначала последовательность кросс-слоев: $$x_{i+1} = x_0 \\odot (W \\times x_i + b) + x_i.$$\n",
        "\n",
        "- Затем последовательность Deep слоев: $$h_{l+1} = f(W_lh_l + b_l).$$\n",
        "\n",
        "\n",
        "### Почему работает и зачем\n",
        "- Значем, что cross признаки важны. \n",
        "- DNN выучивает только неявные взаимодейтсвия, плохо аппроксимирует dot-product => нужные глубокие сети. \n",
        "- CrossNet добавляет явные взаимодействия признаков => не нужны глубоки DNN => может применять в рантайме.\n",
        "\n",
        "<div style=\"width:50%; margin: auto;\">\n",
        "\n",
        "![](https://i.ibb.co/K34HqJH/dcn-theory.png)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvRu94iB-DSt"
      },
      "outputs": [],
      "source": [
        "class CrossLayer(torch.nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x0, xl):\n",
        "        return x0 * self.linear(xl) + xl\n",
        "\n",
        "\n",
        "class CrossNetwork(torch.nn.Module):\n",
        "    def __init__(self, input_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([CrossLayer(input_dim) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        xl = x\n",
        "        for layer in self.layers:\n",
        "            xl = layer(x, xl)\n",
        "        return xl\n",
        "\n",
        "\n",
        "class DeepNetwork(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_units):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for units in hidden_units:\n",
        "            layers.append(nn.Linear(input_dim, units))\n",
        "            layers.append(nn.ReLU())\n",
        "            input_dim = units\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class DCNV2(nn.Module):\n",
        "    def __init__(self, embedding_size, cross_layers, deep_units, input_size, cardinality=65536):\n",
        "        super().__init__()\n",
        "        self.sparse_encode_layer = UnifiedEmbeddings(cardinality, embedding_size)\n",
        "        self.dense_encode_layer = PiecewiseLinearEncoding() \n",
        "        self.cross_network = CrossNetwork(input_size, cross_layers)\n",
        "        self.deep_network = DeepNetwork(input_size, deep_units)\n",
        "        self.output_layer = nn.Linear(deep_units[-1], 1)\n",
        "\n",
        "    def forward(self, dense_input, sparse_input):\n",
        "        sparse_embeddings = self.sparse_encode_layer(sparse_input).view(sparse_input.size(0), -1)\n",
        "        dense_embeddings = self.dense_encode_layer(dense_input)\n",
        "        combined_input = torch.cat([dense_embeddings, sparse_embeddings], dim=-1)\n",
        "        cross_output = self.cross_network(combined_input)\n",
        "        deep_output = self.deep_network(cross_output)\n",
        "        return self.output_layer(deep_output).squeeze(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGt5jk-Zv3fW"
      },
      "source": [
        "## 5. Обучаем нейросетевое ранжирование"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7SqHAgI6DbA_"
      },
      "outputs": [],
      "source": [
        "class CriteoDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            df: pl.DataFrame,\n",
        "            transforms: list[tp.Callable[[tp.Any], tp.Any]] = None,\n",
        "    ):\n",
        "        self._labels = df[CriteoDatasetUtils.LABEL_COl].to_torch().to(torch.float32)\n",
        "        self._dense = df[CriteoDatasetUtils.INT_COLS].to_torch()\n",
        "        self._sparse = df[CriteoDatasetUtils.CAT_COLS].to_torch()\n",
        "        self._transforms = (\n",
        "            transforms if transforms is not None else []\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._labels.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {\n",
        "            'label': self._labels[idx],\n",
        "            'dense_features': self._dense[idx],\n",
        "            'sparse_features': self._sparse[idx]\n",
        "        }\n",
        "        for transform in self._transforms:\n",
        "            sample = transform(sample)\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 4096\n",
        "cardinality = 8 * 65536\n",
        "seeds = [[2342 + 13 * i, 7777 + 17 * i, 131 + 833 * i] for i in range(len(CriteoDatasetUtils.CAT_COLS))]\n",
        "num_hashes = 3\n",
        "embedding_size = 64 \n",
        "n_bins = 39 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LagiiLf50Yer"
      },
      "outputs": [],
      "source": [
        "dense_transform = PiecewiseLinearEncodingTransform(train_df[CriteoDatasetUtils.INT_COLS], n_bins, name='dense_features')\n",
        "sparse_transform = MultihashTransform(cardinality, seeds, name='sparse_features')\n",
        "transforms = [dense_transform, sparse_transform]\n",
        "\n",
        "train_dataset = CriteoDataset(train_df, transforms)\n",
        "val_dataset = CriteoDataset(test_df, transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rMCqyw4qzw7H"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=5, lr=0.001):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in tqdm(train_loader):\n",
        "            int_features, cat_features, labels = batch['dense_features'].to(device), batch['sparse_features'].to(device), batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(int_features, cat_features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0, 0, 0\n",
        "        all_scores, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader):\n",
        "                int_features, cat_features, labels = batch['dense_features'].to(device), batch['sparse_features'].to(device), batch['label'].to(device)\n",
        "\n",
        "                outputs = model(int_features, cat_features)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                all_scores.append(outputs.clone().cpu())\n",
        "                all_labels.append(labels.clone().cpu())\n",
        "            all_scores = torch.cat(all_scores, dim=-1)\n",
        "            all_labels = torch.cat(all_labels, dim=-1)\n",
        "\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, '\n",
        "              f'Val Loss: {val_loss/len(val_loader):.4f}, Accuracy: {100*correct/total:.2f}%, '\n",
        "              f'Val ROC AUC: {roc_auc_score(all_labels, all_scores)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "i6HlVLei0Fl5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5486\n"
          ]
        }
      ],
      "source": [
        "input_size = max(dense_transform.n_bins) * len(CriteoDatasetUtils.INT_COLS) + num_hashes * embedding_size * len(CriteoDatasetUtils.CAT_COLS)\n",
        "model = DCNV2(\n",
        "    embedding_size=embedding_size,\n",
        "    cross_layers=3,\n",
        "    deep_units=[1024, 1024, 1024],\n",
        "    input_size=input_size,\n",
        "    cardinality=cardinality,\n",
        ")\n",
        "print(input_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "bgDLiwnx3DPE",
        "outputId": "e6c3907c-f4a0-4d4a-c63f-11fa99286450"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 960/960 [07:09<00:00,  2.23it/s]\n",
            "100%|██████████| 160/160 [01:10<00:00,  2.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Train Loss: 0.4658, Val Loss: 0.4616, Accuracy: 77.83%, Val ROC AUC: 0.7907098869750834\n"
          ]
        }
      ],
      "source": [
        "train_model(model, train_loader, val_loader, epochs=1, lr=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz2Ec80fwU3l"
      },
      "source": [
        "## 6. Сравниваем с катбустом на том же наборе признаков"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6YcTIK0HleqM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 26021.1875 Total: 81153.75\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate set to 0.035238\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\ttest: 0.7171938\tbest: 0.7171938 (0)\ttotal: 537ms\tremaining: 8m 56s\n",
            "1:\ttotal: 844ms\tremaining: 7m 1s\n",
            "2:\ttotal: 1.17s\tremaining: 6m 30s\n",
            "3:\ttotal: 1.4s\tremaining: 5m 49s\n",
            "4:\ttotal: 1.63s\tremaining: 5m 25s\n",
            "5:\ttest: 0.7310488\tbest: 0.7310488 (5)\ttotal: 2.13s\tremaining: 5m 52s\n",
            "6:\ttotal: 2.29s\tremaining: 5m 24s\n",
            "7:\ttotal: 2.53s\tremaining: 5m 13s\n",
            "8:\ttotal: 2.78s\tremaining: 5m 6s\n",
            "9:\ttotal: 3.02s\tremaining: 4m 58s\n",
            "10:\ttest: 0.7424860\tbest: 0.7424860 (10)\ttotal: 3.33s\tremaining: 4m 59s\n",
            "11:\ttotal: 3.57s\tremaining: 4m 54s\n",
            "12:\ttotal: 4.09s\tremaining: 5m 10s\n",
            "13:\ttotal: 4.33s\tremaining: 5m 4s\n",
            "14:\ttotal: 4.56s\tremaining: 4m 59s\n",
            "15:\ttest: 0.7461342\tbest: 0.7461342 (15)\ttotal: 4.74s\tremaining: 4m 51s\n",
            "16:\ttotal: 4.94s\tremaining: 4m 45s\n",
            "17:\ttotal: 5.39s\tremaining: 4m 53s\n",
            "18:\ttotal: 5.74s\tremaining: 4m 56s\n",
            "19:\ttotal: 5.95s\tremaining: 4m 51s\n",
            "20:\ttest: 0.7499118\tbest: 0.7499118 (20)\ttotal: 6.21s\tremaining: 4m 49s\n",
            "21:\ttotal: 6.42s\tremaining: 4m 45s\n",
            "22:\ttotal: 6.63s\tremaining: 4m 41s\n",
            "23:\ttotal: 6.95s\tremaining: 4m 42s\n",
            "24:\ttotal: 7.29s\tremaining: 4m 44s\n",
            "25:\ttest: 0.7527254\tbest: 0.7527254 (25)\ttotal: 7.63s\tremaining: 4m 45s\n",
            "26:\ttotal: 7.8s\tremaining: 4m 41s\n",
            "27:\ttotal: 8.02s\tremaining: 4m 38s\n",
            "28:\ttotal: 8.29s\tremaining: 4m 37s\n",
            "29:\ttotal: 8.49s\tremaining: 4m 34s\n",
            "30:\ttest: 0.7553181\tbest: 0.7553181 (30)\ttotal: 8.73s\tremaining: 4m 32s\n",
            "31:\ttotal: 8.97s\tremaining: 4m 31s\n",
            "32:\ttotal: 9.2s\tremaining: 4m 29s\n",
            "33:\ttotal: 9.45s\tremaining: 4m 28s\n",
            "34:\ttotal: 9.64s\tremaining: 4m 25s\n",
            "35:\ttest: 0.7574898\tbest: 0.7574898 (35)\ttotal: 9.89s\tremaining: 4m 24s\n",
            "36:\ttotal: 10.1s\tremaining: 4m 23s\n",
            "37:\ttotal: 10.4s\tremaining: 4m 22s\n",
            "38:\ttotal: 10.6s\tremaining: 4m 21s\n",
            "39:\ttotal: 10.8s\tremaining: 4m 20s\n",
            "40:\ttest: 0.7589086\tbest: 0.7589086 (40)\ttotal: 11.1s\tremaining: 4m 19s\n",
            "41:\ttotal: 11.3s\tremaining: 4m 17s\n",
            "42:\ttotal: 11.6s\tremaining: 4m 17s\n",
            "43:\ttotal: 11.8s\tremaining: 4m 16s\n",
            "44:\ttotal: 12.1s\tremaining: 4m 16s\n",
            "45:\ttest: 0.7602838\tbest: 0.7602838 (45)\ttotal: 12.3s\tremaining: 4m 15s\n",
            "46:\ttotal: 12.6s\tremaining: 4m 14s\n",
            "47:\ttotal: 12.8s\tremaining: 4m 13s\n",
            "48:\ttotal: 13s\tremaining: 4m 12s\n",
            "49:\ttotal: 13.2s\tremaining: 4m 11s\n",
            "50:\ttest: 0.7616951\tbest: 0.7616951 (50)\ttotal: 13.5s\tremaining: 4m 12s\n",
            "51:\ttotal: 13.8s\tremaining: 4m 12s\n",
            "52:\ttotal: 14.1s\tremaining: 4m 11s\n",
            "53:\ttotal: 14.3s\tremaining: 4m 10s\n",
            "54:\ttotal: 14.6s\tremaining: 4m 10s\n",
            "55:\ttest: 0.7628089\tbest: 0.7628089 (55)\ttotal: 15s\tremaining: 4m 12s\n",
            "56:\ttotal: 15.2s\tremaining: 4m 10s\n",
            "57:\ttotal: 15.6s\tremaining: 4m 13s\n",
            "58:\ttotal: 15.9s\tremaining: 4m 13s\n",
            "59:\ttotal: 16.3s\tremaining: 4m 14s\n",
            "60:\ttest: 0.7638158\tbest: 0.7638158 (60)\ttotal: 16.5s\tremaining: 4m 13s\n",
            "61:\ttotal: 16.8s\tremaining: 4m 14s\n",
            "62:\ttotal: 17.1s\tremaining: 4m 14s\n",
            "63:\ttotal: 17.3s\tremaining: 4m 12s\n",
            "64:\ttotal: 17.5s\tremaining: 4m 11s\n",
            "65:\ttest: 0.7647552\tbest: 0.7647552 (65)\ttotal: 17.7s\tremaining: 4m 10s\n",
            "66:\ttotal: 18s\tremaining: 4m 10s\n",
            "67:\ttotal: 18.2s\tremaining: 4m 9s\n",
            "68:\ttotal: 18.5s\tremaining: 4m 9s\n",
            "69:\ttotal: 18.7s\tremaining: 4m 7s\n",
            "70:\ttest: 0.7657200\tbest: 0.7657200 (70)\ttotal: 19s\tremaining: 4m 9s\n",
            "71:\ttotal: 19.2s\tremaining: 4m 7s\n",
            "72:\ttotal: 19.6s\tremaining: 4m 9s\n",
            "73:\ttotal: 19.8s\tremaining: 4m 8s\n",
            "74:\ttotal: 20.1s\tremaining: 4m 7s\n",
            "75:\ttest: 0.7664647\tbest: 0.7664647 (75)\ttotal: 20.4s\tremaining: 4m 8s\n",
            "76:\ttotal: 20.9s\tremaining: 4m 10s\n",
            "77:\ttotal: 21.3s\tremaining: 4m 11s\n",
            "78:\ttotal: 21.7s\tremaining: 4m 13s\n",
            "79:\ttotal: 22s\tremaining: 4m 12s\n",
            "80:\ttest: 0.7671964\tbest: 0.7671964 (80)\ttotal: 22.3s\tremaining: 4m 12s\n",
            "81:\ttotal: 22.7s\tremaining: 4m 13s\n",
            "82:\ttotal: 23s\tremaining: 4m 14s\n",
            "83:\ttotal: 23.3s\tremaining: 4m 13s\n",
            "84:\ttotal: 23.4s\tremaining: 4m 12s\n",
            "85:\ttest: 0.7678959\tbest: 0.7678959 (85)\ttotal: 23.8s\tremaining: 4m 13s\n",
            "86:\ttotal: 24.1s\tremaining: 4m 13s\n",
            "87:\ttotal: 24.3s\tremaining: 4m 11s\n",
            "88:\ttotal: 24.6s\tremaining: 4m 11s\n",
            "89:\ttotal: 24.8s\tremaining: 4m 10s\n",
            "90:\ttest: 0.7685435\tbest: 0.7685435 (90)\ttotal: 24.9s\tremaining: 4m 9s\n",
            "91:\ttotal: 25.2s\tremaining: 4m 8s\n",
            "92:\ttotal: 25.6s\tremaining: 4m 9s\n",
            "93:\ttotal: 25.8s\tremaining: 4m 8s\n",
            "94:\ttotal: 26.1s\tremaining: 4m 8s\n",
            "95:\ttest: 0.7691273\tbest: 0.7691273 (95)\ttotal: 26.3s\tremaining: 4m 7s\n",
            "96:\ttotal: 26.4s\tremaining: 4m 6s\n",
            "97:\ttotal: 26.6s\tremaining: 4m 5s\n",
            "98:\ttotal: 26.9s\tremaining: 4m 5s\n",
            "99:\ttotal: 27.2s\tremaining: 4m 5s\n",
            "100:\ttest: 0.7698499\tbest: 0.7698499 (100)\ttotal: 27.5s\tremaining: 4m 4s\n",
            "101:\ttotal: 27.7s\tremaining: 4m 3s\n",
            "102:\ttotal: 28.1s\tremaining: 4m 4s\n",
            "103:\ttotal: 28.2s\tremaining: 4m 3s\n",
            "104:\ttotal: 28.4s\tremaining: 4m 2s\n",
            "105:\ttest: 0.7703857\tbest: 0.7703857 (105)\ttotal: 28.9s\tremaining: 4m 3s\n",
            "106:\ttotal: 29s\tremaining: 4m 2s\n",
            "107:\ttotal: 29.3s\tremaining: 4m 1s\n",
            "108:\ttotal: 29.5s\tremaining: 4m 1s\n",
            "109:\ttotal: 29.7s\tremaining: 4m\n",
            "110:\ttest: 0.7710643\tbest: 0.7710643 (110)\ttotal: 30s\tremaining: 4m\n",
            "111:\ttotal: 30.1s\tremaining: 3m 58s\n",
            "112:\ttotal: 30.3s\tremaining: 3m 58s\n",
            "113:\ttotal: 30.5s\tremaining: 3m 57s\n",
            "114:\ttotal: 30.7s\tremaining: 3m 56s\n",
            "115:\ttest: 0.7716814\tbest: 0.7716814 (115)\ttotal: 31s\tremaining: 3m 56s\n",
            "116:\ttotal: 31.2s\tremaining: 3m 55s\n",
            "117:\ttotal: 31.4s\tremaining: 3m 54s\n",
            "118:\ttotal: 31.7s\tremaining: 3m 54s\n",
            "119:\ttotal: 31.9s\tremaining: 3m 54s\n",
            "120:\ttest: 0.7721554\tbest: 0.7721554 (120)\ttotal: 32.1s\tremaining: 3m 53s\n",
            "121:\ttotal: 32.4s\tremaining: 3m 52s\n",
            "122:\ttotal: 32.6s\tremaining: 3m 52s\n",
            "123:\ttotal: 32.8s\tremaining: 3m 51s\n",
            "124:\ttotal: 33s\tremaining: 3m 51s\n",
            "125:\ttest: 0.7725733\tbest: 0.7725733 (125)\ttotal: 33.2s\tremaining: 3m 50s\n",
            "126:\ttotal: 33.4s\tremaining: 3m 49s\n",
            "127:\ttotal: 33.5s\tremaining: 3m 48s\n",
            "128:\ttotal: 33.7s\tremaining: 3m 47s\n",
            "129:\ttotal: 33.9s\tremaining: 3m 46s\n",
            "130:\ttest: 0.7730993\tbest: 0.7730993 (130)\ttotal: 34.1s\tremaining: 3m 45s\n",
            "131:\ttotal: 34.3s\tremaining: 3m 45s\n",
            "132:\ttotal: 34.5s\tremaining: 3m 44s\n",
            "133:\ttotal: 34.7s\tremaining: 3m 44s\n",
            "134:\ttotal: 35s\tremaining: 3m 44s\n",
            "135:\ttest: 0.7734705\tbest: 0.7734705 (135)\ttotal: 35.2s\tremaining: 3m 43s\n",
            "136:\ttotal: 35.4s\tremaining: 3m 43s\n",
            "137:\ttotal: 35.7s\tremaining: 3m 42s\n",
            "138:\ttotal: 35.9s\tremaining: 3m 42s\n",
            "139:\ttotal: 36.2s\tremaining: 3m 42s\n",
            "140:\ttest: 0.7738192\tbest: 0.7738192 (140)\ttotal: 36.3s\tremaining: 3m 41s\n",
            "141:\ttotal: 36.9s\tremaining: 3m 42s\n",
            "142:\ttotal: 37.1s\tremaining: 3m 42s\n",
            "143:\ttotal: 37.4s\tremaining: 3m 42s\n",
            "144:\ttotal: 37.5s\tremaining: 3m 41s\n",
            "145:\ttest: 0.7741842\tbest: 0.7741842 (145)\ttotal: 37.7s\tremaining: 3m 40s\n",
            "146:\ttotal: 37.9s\tremaining: 3m 39s\n",
            "147:\ttotal: 38.1s\tremaining: 3m 39s\n",
            "148:\ttotal: 38.3s\tremaining: 3m 38s\n",
            "149:\ttotal: 38.5s\tremaining: 3m 37s\n",
            "150:\ttest: 0.7746826\tbest: 0.7746826 (150)\ttotal: 38.8s\tremaining: 3m 38s\n",
            "151:\ttotal: 39.2s\tremaining: 3m 38s\n",
            "152:\ttotal: 39.6s\tremaining: 3m 39s\n",
            "153:\ttotal: 39.8s\tremaining: 3m 38s\n",
            "154:\ttotal: 40.2s\tremaining: 3m 39s\n",
            "155:\ttest: 0.7750742\tbest: 0.7750742 (155)\ttotal: 40.4s\tremaining: 3m 38s\n",
            "156:\ttotal: 40.8s\tremaining: 3m 39s\n",
            "157:\ttotal: 41.1s\tremaining: 3m 39s\n",
            "158:\ttotal: 41.5s\tremaining: 3m 39s\n",
            "159:\ttotal: 42s\tremaining: 3m 40s\n",
            "160:\ttest: 0.7753572\tbest: 0.7753572 (160)\ttotal: 42.2s\tremaining: 3m 40s\n",
            "161:\ttotal: 42.6s\tremaining: 3m 40s\n",
            "162:\ttotal: 42.7s\tremaining: 3m 39s\n",
            "163:\ttotal: 43.2s\tremaining: 3m 40s\n",
            "164:\ttotal: 43.7s\tremaining: 3m 40s\n",
            "165:\ttest: 0.7756603\tbest: 0.7756603 (165)\ttotal: 43.9s\tremaining: 3m 40s\n",
            "166:\ttotal: 44.2s\tremaining: 3m 40s\n",
            "167:\ttotal: 44.4s\tremaining: 3m 39s\n",
            "168:\ttotal: 44.7s\tremaining: 3m 39s\n",
            "169:\ttotal: 44.9s\tremaining: 3m 39s\n",
            "170:\ttest: 0.7760995\tbest: 0.7760995 (170)\ttotal: 45.1s\tremaining: 3m 38s\n",
            "171:\ttotal: 45.5s\tremaining: 3m 39s\n",
            "172:\ttotal: 45.7s\tremaining: 3m 38s\n",
            "173:\ttotal: 45.9s\tremaining: 3m 38s\n",
            "174:\ttotal: 46.1s\tremaining: 3m 37s\n",
            "175:\ttest: 0.7763218\tbest: 0.7763218 (175)\ttotal: 46.3s\tremaining: 3m 36s\n",
            "176:\ttotal: 46.6s\tremaining: 3m 36s\n",
            "177:\ttotal: 46.8s\tremaining: 3m 35s\n",
            "178:\ttotal: 47.3s\tremaining: 3m 36s\n",
            "179:\ttotal: 47.6s\tremaining: 3m 36s\n",
            "180:\ttest: 0.7766129\tbest: 0.7766129 (180)\ttotal: 47.7s\tremaining: 3m 36s\n",
            "181:\ttotal: 48s\tremaining: 3m 35s\n",
            "182:\ttotal: 48.2s\tremaining: 3m 35s\n",
            "183:\ttotal: 48.5s\tremaining: 3m 34s\n",
            "184:\ttotal: 48.7s\tremaining: 3m 34s\n",
            "185:\ttest: 0.7768361\tbest: 0.7768361 (185)\ttotal: 49.3s\tremaining: 3m 35s\n",
            "186:\ttotal: 49.8s\tremaining: 3m 36s\n",
            "187:\ttotal: 50.1s\tremaining: 3m 36s\n",
            "188:\ttotal: 50.3s\tremaining: 3m 35s\n",
            "189:\ttotal: 50.8s\tremaining: 3m 36s\n",
            "190:\ttest: 0.7770582\tbest: 0.7770582 (190)\ttotal: 51s\tremaining: 3m 35s\n",
            "191:\ttotal: 51.2s\tremaining: 3m 35s\n",
            "192:\ttotal: 51.4s\tremaining: 3m 34s\n",
            "193:\ttotal: 51.6s\tremaining: 3m 34s\n",
            "194:\ttotal: 51.7s\tremaining: 3m 33s\n",
            "195:\ttest: 0.7773784\tbest: 0.7773784 (195)\ttotal: 52s\tremaining: 3m 33s\n",
            "196:\ttotal: 52.5s\tremaining: 3m 33s\n",
            "197:\ttotal: 52.6s\tremaining: 3m 33s\n",
            "198:\ttotal: 52.9s\tremaining: 3m 32s\n",
            "199:\ttotal: 53s\tremaining: 3m 32s\n",
            "200:\ttest: 0.7775349\tbest: 0.7775349 (200)\ttotal: 53.2s\tremaining: 3m 31s\n",
            "201:\ttotal: 53.4s\tremaining: 3m 31s\n",
            "202:\ttotal: 53.8s\tremaining: 3m 31s\n",
            "203:\ttotal: 54s\tremaining: 3m 30s\n",
            "204:\ttotal: 54.2s\tremaining: 3m 30s\n",
            "205:\ttest: 0.7777851\tbest: 0.7777851 (205)\ttotal: 54.3s\tremaining: 3m 29s\n",
            "206:\ttotal: 54.8s\tremaining: 3m 29s\n",
            "207:\ttotal: 55.4s\tremaining: 3m 30s\n",
            "208:\ttotal: 55.9s\tremaining: 3m 31s\n",
            "209:\ttotal: 56.2s\tremaining: 3m 31s\n",
            "210:\ttest: 0.7779725\tbest: 0.7779725 (210)\ttotal: 56.4s\tremaining: 3m 30s\n",
            "211:\ttotal: 57.1s\tremaining: 3m 32s\n",
            "212:\ttotal: 57.4s\tremaining: 3m 31s\n",
            "213:\ttotal: 57.7s\tremaining: 3m 31s\n",
            "214:\ttotal: 58s\tremaining: 3m 31s\n",
            "215:\ttest: 0.7782224\tbest: 0.7782224 (215)\ttotal: 58.4s\tremaining: 3m 31s\n",
            "216:\ttotal: 58.9s\tremaining: 3m 32s\n",
            "217:\ttotal: 59.1s\tremaining: 3m 32s\n",
            "218:\ttotal: 59.5s\tremaining: 3m 32s\n",
            "219:\ttotal: 59.7s\tremaining: 3m 31s\n",
            "220:\ttest: 0.7784140\tbest: 0.7784140 (220)\ttotal: 1m\tremaining: 3m 32s\n",
            "221:\ttotal: 1m\tremaining: 3m 32s\n",
            "222:\ttotal: 1m\tremaining: 3m 31s\n",
            "223:\ttotal: 1m 1s\tremaining: 3m 31s\n",
            "224:\ttotal: 1m 1s\tremaining: 3m 31s\n",
            "225:\ttest: 0.7786427\tbest: 0.7786427 (225)\ttotal: 1m 1s\tremaining: 3m 30s\n",
            "226:\ttotal: 1m 2s\tremaining: 3m 31s\n",
            "227:\ttotal: 1m 2s\tremaining: 3m 30s\n",
            "228:\ttotal: 1m 2s\tremaining: 3m 30s\n",
            "229:\ttotal: 1m 2s\tremaining: 3m 29s\n",
            "230:\ttest: 0.7788157\tbest: 0.7788157 (230)\ttotal: 1m 2s\tremaining: 3m 29s\n",
            "231:\ttotal: 1m 2s\tremaining: 3m 28s\n",
            "232:\ttotal: 1m 3s\tremaining: 3m 28s\n",
            "233:\ttotal: 1m 3s\tremaining: 3m 28s\n",
            "234:\ttotal: 1m 4s\tremaining: 3m 28s\n",
            "235:\ttest: 0.7790194\tbest: 0.7790194 (235)\ttotal: 1m 4s\tremaining: 3m 28s\n",
            "236:\ttotal: 1m 4s\tremaining: 3m 27s\n",
            "237:\ttotal: 1m 4s\tremaining: 3m 27s\n",
            "238:\ttotal: 1m 5s\tremaining: 3m 27s\n",
            "239:\ttotal: 1m 5s\tremaining: 3m 27s\n",
            "240:\ttest: 0.7792413\tbest: 0.7792413 (240)\ttotal: 1m 5s\tremaining: 3m 26s\n",
            "241:\ttotal: 1m 5s\tremaining: 3m 26s\n",
            "242:\ttotal: 1m 5s\tremaining: 3m 25s\n",
            "243:\ttotal: 1m 6s\tremaining: 3m 24s\n",
            "244:\ttotal: 1m 6s\tremaining: 3m 25s\n",
            "245:\ttest: 0.7794374\tbest: 0.7794374 (245)\ttotal: 1m 6s\tremaining: 3m 24s\n",
            "246:\ttotal: 1m 7s\tremaining: 3m 24s\n",
            "247:\ttotal: 1m 7s\tremaining: 3m 23s\n",
            "248:\ttotal: 1m 7s\tremaining: 3m 23s\n",
            "249:\ttotal: 1m 7s\tremaining: 3m 23s\n",
            "250:\ttest: 0.7795708\tbest: 0.7795708 (250)\ttotal: 1m 7s\tremaining: 3m 22s\n",
            "251:\ttotal: 1m 8s\tremaining: 3m 22s\n",
            "252:\ttotal: 1m 8s\tremaining: 3m 22s\n",
            "253:\ttotal: 1m 8s\tremaining: 3m 22s\n",
            "254:\ttotal: 1m 8s\tremaining: 3m 21s\n",
            "255:\ttest: 0.7798122\tbest: 0.7798122 (255)\ttotal: 1m 9s\tremaining: 3m 21s\n",
            "256:\ttotal: 1m 9s\tremaining: 3m 20s\n",
            "257:\ttotal: 1m 9s\tremaining: 3m 19s\n",
            "258:\ttotal: 1m 9s\tremaining: 3m 19s\n",
            "259:\ttotal: 1m 9s\tremaining: 3m 18s\n",
            "260:\ttest: 0.7800159\tbest: 0.7800159 (260)\ttotal: 1m 10s\tremaining: 3m 19s\n",
            "261:\ttotal: 1m 10s\tremaining: 3m 18s\n",
            "262:\ttotal: 1m 10s\tremaining: 3m 18s\n",
            "263:\ttotal: 1m 10s\tremaining: 3m 17s\n",
            "264:\ttotal: 1m 11s\tremaining: 3m 17s\n",
            "265:\ttest: 0.7801977\tbest: 0.7801977 (265)\ttotal: 1m 11s\tremaining: 3m 16s\n",
            "266:\ttotal: 1m 11s\tremaining: 3m 16s\n",
            "267:\ttotal: 1m 11s\tremaining: 3m 15s\n",
            "268:\ttotal: 1m 12s\tremaining: 3m 15s\n",
            "269:\ttotal: 1m 12s\tremaining: 3m 15s\n",
            "270:\ttest: 0.7803192\tbest: 0.7803192 (270)\ttotal: 1m 12s\tremaining: 3m 14s\n",
            "271:\ttotal: 1m 12s\tremaining: 3m 14s\n",
            "272:\ttotal: 1m 12s\tremaining: 3m 13s\n",
            "273:\ttotal: 1m 13s\tremaining: 3m 13s\n",
            "274:\ttotal: 1m 13s\tremaining: 3m 12s\n",
            "275:\ttest: 0.7804864\tbest: 0.7804864 (275)\ttotal: 1m 13s\tremaining: 3m 12s\n",
            "276:\ttotal: 1m 13s\tremaining: 3m 12s\n",
            "277:\ttotal: 1m 14s\tremaining: 3m 12s\n",
            "278:\ttotal: 1m 14s\tremaining: 3m 11s\n",
            "279:\ttotal: 1m 14s\tremaining: 3m 11s\n",
            "280:\ttest: 0.7806512\tbest: 0.7806512 (280)\ttotal: 1m 14s\tremaining: 3m 10s\n",
            "281:\ttotal: 1m 14s\tremaining: 3m 10s\n",
            "282:\ttotal: 1m 14s\tremaining: 3m 9s\n",
            "283:\ttotal: 1m 15s\tremaining: 3m 10s\n",
            "284:\ttotal: 1m 15s\tremaining: 3m 9s\n",
            "285:\ttest: 0.7807918\tbest: 0.7807918 (285)\ttotal: 1m 15s\tremaining: 3m 9s\n",
            "286:\ttotal: 1m 16s\tremaining: 3m 8s\n",
            "287:\ttotal: 1m 16s\tremaining: 3m 8s\n",
            "288:\ttotal: 1m 16s\tremaining: 3m 8s\n",
            "289:\ttotal: 1m 17s\tremaining: 3m 8s\n",
            "290:\ttest: 0.7809158\tbest: 0.7809158 (290)\ttotal: 1m 17s\tremaining: 3m 9s\n",
            "291:\ttotal: 1m 17s\tremaining: 3m 8s\n",
            "292:\ttotal: 1m 18s\tremaining: 3m 8s\n",
            "293:\ttotal: 1m 18s\tremaining: 3m 7s\n",
            "294:\ttotal: 1m 18s\tremaining: 3m 8s\n",
            "295:\ttest: 0.7810796\tbest: 0.7810796 (295)\ttotal: 1m 19s\tremaining: 3m 8s\n",
            "296:\ttotal: 1m 19s\tremaining: 3m 7s\n",
            "297:\ttotal: 1m 19s\tremaining: 3m 7s\n",
            "298:\ttotal: 1m 20s\tremaining: 3m 8s\n",
            "299:\ttotal: 1m 20s\tremaining: 3m 7s\n",
            "300:\ttest: 0.7812065\tbest: 0.7812065 (300)\ttotal: 1m 20s\tremaining: 3m 7s\n",
            "301:\ttotal: 1m 21s\tremaining: 3m 7s\n",
            "302:\ttotal: 1m 21s\tremaining: 3m 7s\n",
            "303:\ttotal: 1m 21s\tremaining: 3m 7s\n",
            "304:\ttotal: 1m 22s\tremaining: 3m 7s\n",
            "305:\ttest: 0.7813289\tbest: 0.7813289 (305)\ttotal: 1m 22s\tremaining: 3m 6s\n",
            "306:\ttotal: 1m 22s\tremaining: 3m 6s\n",
            "307:\ttotal: 1m 22s\tremaining: 3m 5s\n",
            "308:\ttotal: 1m 23s\tremaining: 3m 5s\n",
            "309:\ttotal: 1m 23s\tremaining: 3m 5s\n",
            "310:\ttest: 0.7814480\tbest: 0.7814480 (310)\ttotal: 1m 23s\tremaining: 3m 4s\n",
            "311:\ttotal: 1m 23s\tremaining: 3m 4s\n",
            "312:\ttotal: 1m 23s\tremaining: 3m 4s\n",
            "313:\ttotal: 1m 24s\tremaining: 3m 3s\n",
            "314:\ttotal: 1m 24s\tremaining: 3m 3s\n",
            "315:\ttest: 0.7815543\tbest: 0.7815543 (315)\ttotal: 1m 24s\tremaining: 3m 2s\n",
            "316:\ttotal: 1m 24s\tremaining: 3m 3s\n",
            "317:\ttotal: 1m 25s\tremaining: 3m 2s\n",
            "318:\ttotal: 1m 25s\tremaining: 3m 2s\n",
            "319:\ttotal: 1m 25s\tremaining: 3m 1s\n",
            "320:\ttest: 0.7816840\tbest: 0.7816840 (320)\ttotal: 1m 25s\tremaining: 3m 1s\n",
            "321:\ttotal: 1m 26s\tremaining: 3m 1s\n",
            "322:\ttotal: 1m 26s\tremaining: 3m\n",
            "323:\ttotal: 1m 26s\tremaining: 3m\n",
            "324:\ttotal: 1m 26s\tremaining: 2m 59s\n",
            "325:\ttest: 0.7818185\tbest: 0.7818185 (325)\ttotal: 1m 26s\tremaining: 2m 59s\n",
            "326:\ttotal: 1m 26s\tremaining: 2m 59s\n",
            "327:\ttotal: 1m 27s\tremaining: 2m 58s\n",
            "328:\ttotal: 1m 27s\tremaining: 2m 58s\n",
            "329:\ttotal: 1m 27s\tremaining: 2m 57s\n",
            "330:\ttest: 0.7819634\tbest: 0.7819634 (330)\ttotal: 1m 27s\tremaining: 2m 57s\n",
            "331:\ttotal: 1m 27s\tremaining: 2m 56s\n",
            "332:\ttotal: 1m 28s\tremaining: 2m 57s\n",
            "333:\ttotal: 1m 28s\tremaining: 2m 56s\n",
            "334:\ttotal: 1m 28s\tremaining: 2m 56s\n",
            "335:\ttest: 0.7821069\tbest: 0.7821069 (335)\ttotal: 1m 29s\tremaining: 2m 55s\n",
            "336:\ttotal: 1m 29s\tremaining: 2m 55s\n",
            "337:\ttotal: 1m 29s\tremaining: 2m 54s\n",
            "338:\ttotal: 1m 29s\tremaining: 2m 54s\n",
            "339:\ttotal: 1m 30s\tremaining: 2m 54s\n",
            "340:\ttest: 0.7822428\tbest: 0.7822428 (340)\ttotal: 1m 30s\tremaining: 2m 54s\n",
            "341:\ttotal: 1m 30s\tremaining: 2m 54s\n",
            "342:\ttotal: 1m 30s\tremaining: 2m 53s\n",
            "343:\ttotal: 1m 30s\tremaining: 2m 53s\n",
            "344:\ttotal: 1m 30s\tremaining: 2m 52s\n",
            "345:\ttest: 0.7823912\tbest: 0.7823912 (345)\ttotal: 1m 31s\tremaining: 2m 52s\n",
            "346:\ttotal: 1m 31s\tremaining: 2m 52s\n",
            "347:\ttotal: 1m 32s\tremaining: 2m 52s\n",
            "348:\ttotal: 1m 32s\tremaining: 2m 52s\n",
            "349:\ttotal: 1m 32s\tremaining: 2m 51s\n",
            "350:\ttest: 0.7825039\tbest: 0.7825039 (350)\ttotal: 1m 32s\tremaining: 2m 51s\n",
            "351:\ttotal: 1m 32s\tremaining: 2m 51s\n",
            "352:\ttotal: 1m 33s\tremaining: 2m 51s\n",
            "353:\ttotal: 1m 33s\tremaining: 2m 51s\n",
            "354:\ttotal: 1m 33s\tremaining: 2m 50s\n",
            "355:\ttest: 0.7826101\tbest: 0.7826101 (355)\ttotal: 1m 34s\tremaining: 2m 50s\n",
            "356:\ttotal: 1m 34s\tremaining: 2m 49s\n",
            "357:\ttotal: 1m 34s\tremaining: 2m 49s\n",
            "358:\ttotal: 1m 34s\tremaining: 2m 49s\n",
            "359:\ttotal: 1m 35s\tremaining: 2m 49s\n",
            "360:\ttest: 0.7827567\tbest: 0.7827567 (360)\ttotal: 1m 35s\tremaining: 2m 49s\n",
            "361:\ttotal: 1m 35s\tremaining: 2m 48s\n",
            "362:\ttotal: 1m 36s\tremaining: 2m 48s\n",
            "363:\ttotal: 1m 36s\tremaining: 2m 48s\n",
            "364:\ttotal: 1m 36s\tremaining: 2m 47s\n",
            "365:\ttest: 0.7829080\tbest: 0.7829080 (365)\ttotal: 1m 36s\tremaining: 2m 47s\n",
            "366:\ttotal: 1m 37s\tremaining: 2m 47s\n",
            "367:\ttotal: 1m 37s\tremaining: 2m 47s\n",
            "368:\ttotal: 1m 37s\tremaining: 2m 46s\n",
            "369:\ttotal: 1m 37s\tremaining: 2m 46s\n",
            "370:\ttest: 0.7830228\tbest: 0.7830228 (370)\ttotal: 1m 38s\tremaining: 2m 46s\n",
            "371:\ttotal: 1m 38s\tremaining: 2m 46s\n",
            "372:\ttotal: 1m 38s\tremaining: 2m 45s\n",
            "373:\ttotal: 1m 38s\tremaining: 2m 45s\n",
            "374:\ttotal: 1m 39s\tremaining: 2m 45s\n",
            "375:\ttest: 0.7831249\tbest: 0.7831249 (375)\ttotal: 1m 39s\tremaining: 2m 44s\n",
            "376:\ttotal: 1m 39s\tremaining: 2m 44s\n",
            "377:\ttotal: 1m 39s\tremaining: 2m 44s\n",
            "378:\ttotal: 1m 40s\tremaining: 2m 44s\n",
            "379:\ttotal: 1m 40s\tremaining: 2m 43s\n",
            "380:\ttest: 0.7832293\tbest: 0.7832293 (380)\ttotal: 1m 40s\tremaining: 2m 43s\n",
            "381:\ttotal: 1m 40s\tremaining: 2m 43s\n",
            "382:\ttotal: 1m 41s\tremaining: 2m 42s\n",
            "383:\ttotal: 1m 41s\tremaining: 2m 42s\n",
            "384:\ttotal: 1m 41s\tremaining: 2m 42s\n",
            "385:\ttest: 0.7833527\tbest: 0.7833527 (385)\ttotal: 1m 41s\tremaining: 2m 42s\n",
            "386:\ttotal: 1m 42s\tremaining: 2m 41s\n",
            "387:\ttotal: 1m 42s\tremaining: 2m 41s\n",
            "388:\ttotal: 1m 42s\tremaining: 2m 41s\n",
            "389:\ttotal: 1m 42s\tremaining: 2m 40s\n",
            "390:\ttest: 0.7834583\tbest: 0.7834583 (390)\ttotal: 1m 43s\tremaining: 2m 41s\n",
            "391:\ttotal: 1m 43s\tremaining: 2m 40s\n",
            "392:\ttotal: 1m 43s\tremaining: 2m 40s\n",
            "393:\ttotal: 1m 44s\tremaining: 2m 40s\n",
            "394:\ttotal: 1m 44s\tremaining: 2m 39s\n",
            "395:\ttest: 0.7835907\tbest: 0.7835907 (395)\ttotal: 1m 44s\tremaining: 2m 39s\n",
            "396:\ttotal: 1m 45s\tremaining: 2m 39s\n",
            "397:\ttotal: 1m 45s\tremaining: 2m 39s\n",
            "398:\ttotal: 1m 45s\tremaining: 2m 39s\n",
            "399:\ttotal: 1m 46s\tremaining: 2m 39s\n",
            "400:\ttest: 0.7837206\tbest: 0.7837206 (400)\ttotal: 1m 46s\tremaining: 2m 38s\n",
            "401:\ttotal: 1m 46s\tremaining: 2m 38s\n",
            "402:\ttotal: 1m 47s\tremaining: 2m 38s\n",
            "403:\ttotal: 1m 47s\tremaining: 2m 38s\n",
            "404:\ttotal: 1m 47s\tremaining: 2m 38s\n",
            "405:\ttest: 0.7838297\tbest: 0.7838297 (405)\ttotal: 1m 47s\tremaining: 2m 37s\n",
            "406:\ttotal: 1m 48s\tremaining: 2m 38s\n",
            "407:\ttotal: 1m 48s\tremaining: 2m 38s\n",
            "408:\ttotal: 1m 49s\tremaining: 2m 37s\n",
            "409:\ttotal: 1m 49s\tremaining: 2m 37s\n",
            "410:\ttest: 0.7839489\tbest: 0.7839489 (410)\ttotal: 1m 49s\tremaining: 2m 37s\n",
            "411:\ttotal: 1m 50s\tremaining: 2m 37s\n",
            "412:\ttotal: 1m 50s\tremaining: 2m 37s\n",
            "413:\ttotal: 1m 50s\tremaining: 2m 36s\n",
            "414:\ttotal: 1m 51s\tremaining: 2m 36s\n",
            "415:\ttest: 0.7840545\tbest: 0.7840545 (415)\ttotal: 1m 51s\tremaining: 2m 36s\n",
            "416:\ttotal: 1m 51s\tremaining: 2m 35s\n",
            "417:\ttotal: 1m 51s\tremaining: 2m 35s\n",
            "418:\ttotal: 1m 52s\tremaining: 2m 35s\n",
            "419:\ttotal: 1m 52s\tremaining: 2m 35s\n",
            "420:\ttest: 0.7841321\tbest: 0.7841321 (420)\ttotal: 1m 52s\tremaining: 2m 34s\n",
            "421:\ttotal: 1m 52s\tremaining: 2m 34s\n",
            "422:\ttotal: 1m 53s\tremaining: 2m 34s\n",
            "423:\ttotal: 1m 53s\tremaining: 2m 34s\n",
            "424:\ttotal: 1m 54s\tremaining: 2m 34s\n",
            "425:\ttest: 0.7842301\tbest: 0.7842301 (425)\ttotal: 1m 54s\tremaining: 2m 34s\n",
            "426:\ttotal: 1m 54s\tremaining: 2m 33s\n",
            "427:\ttotal: 1m 54s\tremaining: 2m 33s\n",
            "428:\ttotal: 1m 55s\tremaining: 2m 33s\n",
            "429:\ttotal: 1m 55s\tremaining: 2m 33s\n",
            "430:\ttest: 0.7843698\tbest: 0.7843698 (430)\ttotal: 1m 55s\tremaining: 2m 32s\n",
            "431:\ttotal: 1m 56s\tremaining: 2m 33s\n",
            "432:\ttotal: 1m 56s\tremaining: 2m 32s\n",
            "433:\ttotal: 1m 57s\tremaining: 2m 32s\n",
            "434:\ttotal: 1m 57s\tremaining: 2m 32s\n",
            "435:\ttest: 0.7844613\tbest: 0.7844613 (435)\ttotal: 1m 58s\tremaining: 2m 32s\n",
            "436:\ttotal: 1m 58s\tremaining: 2m 32s\n",
            "437:\ttotal: 1m 58s\tremaining: 2m 32s\n",
            "438:\ttotal: 1m 58s\tremaining: 2m 32s\n",
            "439:\ttotal: 1m 59s\tremaining: 2m 32s\n",
            "440:\ttest: 0.7845359\tbest: 0.7845359 (440)\ttotal: 2m\tremaining: 2m 32s\n",
            "441:\ttotal: 2m\tremaining: 2m 31s\n",
            "442:\ttotal: 2m\tremaining: 2m 31s\n",
            "443:\ttotal: 2m\tremaining: 2m 31s\n",
            "444:\ttotal: 2m 1s\tremaining: 2m 31s\n",
            "445:\ttest: 0.7846122\tbest: 0.7846122 (445)\ttotal: 2m 1s\tremaining: 2m 30s\n",
            "446:\ttotal: 2m 1s\tremaining: 2m 30s\n",
            "447:\ttotal: 2m 1s\tremaining: 2m 30s\n",
            "448:\ttotal: 2m 2s\tremaining: 2m 29s\n",
            "449:\ttotal: 2m 2s\tremaining: 2m 29s\n",
            "450:\ttest: 0.7846885\tbest: 0.7846885 (450)\ttotal: 2m 2s\tremaining: 2m 29s\n",
            "451:\ttotal: 2m 3s\tremaining: 2m 29s\n",
            "452:\ttotal: 2m 3s\tremaining: 2m 29s\n",
            "453:\ttotal: 2m 3s\tremaining: 2m 28s\n",
            "454:\ttotal: 2m 4s\tremaining: 2m 28s\n",
            "455:\ttest: 0.7847754\tbest: 0.7847754 (455)\ttotal: 2m 4s\tremaining: 2m 28s\n",
            "456:\ttotal: 2m 4s\tremaining: 2m 28s\n",
            "457:\ttotal: 2m 4s\tremaining: 2m 27s\n",
            "458:\ttotal: 2m 4s\tremaining: 2m 27s\n",
            "459:\ttotal: 2m 5s\tremaining: 2m 26s\n",
            "460:\ttest: 0.7848556\tbest: 0.7848556 (460)\ttotal: 2m 5s\tremaining: 2m 26s\n",
            "461:\ttotal: 2m 5s\tremaining: 2m 26s\n",
            "462:\ttotal: 2m 5s\tremaining: 2m 25s\n",
            "463:\ttotal: 2m 5s\tremaining: 2m 25s\n",
            "464:\ttotal: 2m 6s\tremaining: 2m 25s\n",
            "465:\ttest: 0.7849472\tbest: 0.7849472 (465)\ttotal: 2m 6s\tremaining: 2m 24s\n",
            "466:\ttotal: 2m 6s\tremaining: 2m 24s\n",
            "467:\ttotal: 2m 7s\tremaining: 2m 24s\n",
            "468:\ttotal: 2m 7s\tremaining: 2m 23s\n",
            "469:\ttotal: 2m 7s\tremaining: 2m 23s\n",
            "470:\ttest: 0.7850281\tbest: 0.7850281 (470)\ttotal: 2m 7s\tremaining: 2m 23s\n",
            "471:\ttotal: 2m 8s\tremaining: 2m 23s\n",
            "472:\ttotal: 2m 8s\tremaining: 2m 23s\n",
            "473:\ttotal: 2m 8s\tremaining: 2m 22s\n",
            "474:\ttotal: 2m 8s\tremaining: 2m 22s\n",
            "475:\ttest: 0.7850954\tbest: 0.7850954 (475)\ttotal: 2m 9s\tremaining: 2m 22s\n",
            "476:\ttotal: 2m 9s\tremaining: 2m 21s\n",
            "477:\ttotal: 2m 9s\tremaining: 2m 21s\n",
            "478:\ttotal: 2m 9s\tremaining: 2m 21s\n",
            "479:\ttotal: 2m 10s\tremaining: 2m 20s\n",
            "480:\ttest: 0.7851732\tbest: 0.7851732 (480)\ttotal: 2m 10s\tremaining: 2m 20s\n",
            "481:\ttotal: 2m 10s\tremaining: 2m 20s\n",
            "482:\ttotal: 2m 10s\tremaining: 2m 20s\n",
            "483:\ttotal: 2m 11s\tremaining: 2m 19s\n",
            "484:\ttotal: 2m 11s\tremaining: 2m 19s\n",
            "485:\ttest: 0.7852331\tbest: 0.7852331 (485)\ttotal: 2m 11s\tremaining: 2m 19s\n",
            "486:\ttotal: 2m 12s\tremaining: 2m 19s\n",
            "487:\ttotal: 2m 12s\tremaining: 2m 18s\n",
            "488:\ttotal: 2m 12s\tremaining: 2m 18s\n",
            "489:\ttotal: 2m 12s\tremaining: 2m 18s\n",
            "490:\ttest: 0.7853060\tbest: 0.7853060 (490)\ttotal: 2m 12s\tremaining: 2m 17s\n",
            "491:\ttotal: 2m 13s\tremaining: 2m 17s\n",
            "492:\ttotal: 2m 13s\tremaining: 2m 17s\n",
            "493:\ttotal: 2m 13s\tremaining: 2m 16s\n",
            "494:\ttotal: 2m 14s\tremaining: 2m 16s\n",
            "495:\ttest: 0.7853704\tbest: 0.7853704 (495)\ttotal: 2m 14s\tremaining: 2m 16s\n",
            "496:\ttotal: 2m 14s\tremaining: 2m 16s\n",
            "497:\ttotal: 2m 15s\tremaining: 2m 16s\n",
            "498:\ttotal: 2m 15s\tremaining: 2m 16s\n",
            "499:\ttotal: 2m 15s\tremaining: 2m 15s\n",
            "500:\ttest: 0.7854505\tbest: 0.7854505 (500)\ttotal: 2m 16s\tremaining: 2m 15s\n",
            "501:\ttotal: 2m 16s\tremaining: 2m 15s\n",
            "502:\ttotal: 2m 16s\tremaining: 2m 14s\n",
            "503:\ttotal: 2m 16s\tremaining: 2m 14s\n",
            "504:\ttotal: 2m 16s\tremaining: 2m 14s\n",
            "505:\ttest: 0.7855224\tbest: 0.7855224 (505)\ttotal: 2m 17s\tremaining: 2m 14s\n",
            "506:\ttotal: 2m 17s\tremaining: 2m 13s\n",
            "507:\ttotal: 2m 18s\tremaining: 2m 13s\n",
            "508:\ttotal: 2m 18s\tremaining: 2m 13s\n",
            "509:\ttotal: 2m 18s\tremaining: 2m 13s\n",
            "510:\ttest: 0.7856103\tbest: 0.7856103 (510)\ttotal: 2m 18s\tremaining: 2m 12s\n",
            "511:\ttotal: 2m 18s\tremaining: 2m 12s\n",
            "512:\ttotal: 2m 19s\tremaining: 2m 12s\n",
            "513:\ttotal: 2m 19s\tremaining: 2m 11s\n",
            "514:\ttotal: 2m 19s\tremaining: 2m 11s\n",
            "515:\ttest: 0.7856975\tbest: 0.7856975 (515)\ttotal: 2m 20s\tremaining: 2m 11s\n",
            "516:\ttotal: 2m 20s\tremaining: 2m 11s\n",
            "517:\ttotal: 2m 20s\tremaining: 2m 10s\n",
            "518:\ttotal: 2m 20s\tremaining: 2m 10s\n",
            "519:\ttotal: 2m 21s\tremaining: 2m 10s\n",
            "520:\ttest: 0.7857679\tbest: 0.7857679 (520)\ttotal: 2m 21s\tremaining: 2m 9s\n",
            "521:\ttotal: 2m 21s\tremaining: 2m 9s\n",
            "522:\ttotal: 2m 21s\tremaining: 2m 9s\n",
            "523:\ttotal: 2m 21s\tremaining: 2m 8s\n",
            "524:\ttotal: 2m 22s\tremaining: 2m 8s\n",
            "525:\ttest: 0.7858230\tbest: 0.7858230 (525)\ttotal: 2m 22s\tremaining: 2m 8s\n",
            "526:\ttotal: 2m 22s\tremaining: 2m 7s\n",
            "527:\ttotal: 2m 22s\tremaining: 2m 7s\n",
            "528:\ttotal: 2m 22s\tremaining: 2m 7s\n",
            "529:\ttotal: 2m 23s\tremaining: 2m 6s\n",
            "530:\ttest: 0.7858816\tbest: 0.7858816 (530)\ttotal: 2m 23s\tremaining: 2m 6s\n",
            "531:\ttotal: 2m 23s\tremaining: 2m 6s\n",
            "532:\ttotal: 2m 23s\tremaining: 2m 5s\n",
            "533:\ttotal: 2m 24s\tremaining: 2m 5s\n",
            "534:\ttotal: 2m 24s\tremaining: 2m 5s\n",
            "535:\ttest: 0.7859735\tbest: 0.7859735 (535)\ttotal: 2m 24s\tremaining: 2m 5s\n",
            "536:\ttotal: 2m 24s\tremaining: 2m 4s\n",
            "537:\ttotal: 2m 25s\tremaining: 2m 4s\n",
            "538:\ttotal: 2m 25s\tremaining: 2m 4s\n",
            "539:\ttotal: 2m 25s\tremaining: 2m 4s\n",
            "540:\ttest: 0.7860299\tbest: 0.7860299 (540)\ttotal: 2m 26s\tremaining: 2m 4s\n",
            "541:\ttotal: 2m 26s\tremaining: 2m 3s\n",
            "542:\ttotal: 2m 26s\tremaining: 2m 3s\n",
            "543:\ttotal: 2m 27s\tremaining: 2m 3s\n",
            "544:\ttotal: 2m 27s\tremaining: 2m 3s\n",
            "545:\ttest: 0.7860968\tbest: 0.7860968 (545)\ttotal: 2m 27s\tremaining: 2m 2s\n",
            "546:\ttotal: 2m 28s\tremaining: 2m 2s\n",
            "547:\ttotal: 2m 28s\tremaining: 2m 2s\n",
            "548:\ttotal: 2m 28s\tremaining: 2m 2s\n",
            "549:\ttotal: 2m 28s\tremaining: 2m 1s\n",
            "550:\ttest: 0.7861546\tbest: 0.7861546 (550)\ttotal: 2m 29s\tremaining: 2m 1s\n",
            "551:\ttotal: 2m 29s\tremaining: 2m 1s\n",
            "552:\ttotal: 2m 29s\tremaining: 2m 1s\n",
            "553:\ttotal: 2m 29s\tremaining: 2m\n",
            "554:\ttotal: 2m 30s\tremaining: 2m\n",
            "555:\ttest: 0.7862149\tbest: 0.7862149 (555)\ttotal: 2m 30s\tremaining: 2m\n",
            "556:\ttotal: 2m 30s\tremaining: 1m 59s\n",
            "557:\ttotal: 2m 31s\tremaining: 1m 59s\n",
            "558:\ttotal: 2m 31s\tremaining: 1m 59s\n",
            "559:\ttotal: 2m 31s\tremaining: 1m 59s\n",
            "560:\ttest: 0.7862787\tbest: 0.7862787 (560)\ttotal: 2m 31s\tremaining: 1m 58s\n",
            "561:\ttotal: 2m 32s\tremaining: 1m 58s\n",
            "562:\ttotal: 2m 32s\tremaining: 1m 58s\n",
            "563:\ttotal: 2m 32s\tremaining: 1m 58s\n",
            "564:\ttotal: 2m 33s\tremaining: 1m 57s\n",
            "565:\ttest: 0.7863470\tbest: 0.7863470 (565)\ttotal: 2m 33s\tremaining: 1m 57s\n",
            "566:\ttotal: 2m 33s\tremaining: 1m 57s\n",
            "567:\ttotal: 2m 33s\tremaining: 1m 57s\n",
            "568:\ttotal: 2m 34s\tremaining: 1m 56s\n",
            "569:\ttotal: 2m 34s\tremaining: 1m 56s\n",
            "570:\ttest: 0.7864119\tbest: 0.7864119 (570)\ttotal: 2m 34s\tremaining: 1m 56s\n",
            "571:\ttotal: 2m 35s\tremaining: 1m 56s\n",
            "572:\ttotal: 2m 35s\tremaining: 1m 55s\n",
            "573:\ttotal: 2m 35s\tremaining: 1m 55s\n",
            "574:\ttotal: 2m 35s\tremaining: 1m 55s\n",
            "575:\ttest: 0.7864693\tbest: 0.7864693 (575)\ttotal: 2m 36s\tremaining: 1m 54s\n",
            "576:\ttotal: 2m 36s\tremaining: 1m 54s\n",
            "577:\ttotal: 2m 36s\tremaining: 1m 54s\n",
            "578:\ttotal: 2m 36s\tremaining: 1m 54s\n",
            "579:\ttotal: 2m 37s\tremaining: 1m 53s\n",
            "580:\ttest: 0.7865564\tbest: 0.7865564 (580)\ttotal: 2m 37s\tremaining: 1m 53s\n",
            "581:\ttotal: 2m 37s\tremaining: 1m 53s\n",
            "582:\ttotal: 2m 38s\tremaining: 1m 53s\n",
            "583:\ttotal: 2m 38s\tremaining: 1m 52s\n",
            "584:\ttotal: 2m 38s\tremaining: 1m 52s\n",
            "585:\ttest: 0.7866203\tbest: 0.7866203 (585)\ttotal: 2m 39s\tremaining: 1m 52s\n",
            "586:\ttotal: 2m 39s\tremaining: 1m 52s\n",
            "587:\ttotal: 2m 39s\tremaining: 1m 51s\n",
            "588:\ttotal: 2m 39s\tremaining: 1m 51s\n",
            "589:\ttotal: 2m 40s\tremaining: 1m 51s\n",
            "590:\ttest: 0.7866766\tbest: 0.7866766 (590)\ttotal: 2m 40s\tremaining: 1m 50s\n",
            "591:\ttotal: 2m 40s\tremaining: 1m 50s\n",
            "592:\ttotal: 2m 40s\tremaining: 1m 50s\n",
            "593:\ttotal: 2m 41s\tremaining: 1m 50s\n",
            "594:\ttotal: 2m 41s\tremaining: 1m 49s\n",
            "595:\ttest: 0.7867385\tbest: 0.7867385 (595)\ttotal: 2m 41s\tremaining: 1m 49s\n",
            "596:\ttotal: 2m 41s\tremaining: 1m 49s\n",
            "597:\ttotal: 2m 41s\tremaining: 1m 48s\n",
            "598:\ttotal: 2m 42s\tremaining: 1m 48s\n",
            "599:\ttotal: 2m 42s\tremaining: 1m 48s\n",
            "600:\ttest: 0.7867947\tbest: 0.7867947 (600)\ttotal: 2m 42s\tremaining: 1m 48s\n",
            "601:\ttotal: 2m 43s\tremaining: 1m 47s\n",
            "602:\ttotal: 2m 43s\tremaining: 1m 47s\n",
            "603:\ttotal: 2m 43s\tremaining: 1m 47s\n",
            "604:\ttotal: 2m 43s\tremaining: 1m 47s\n",
            "605:\ttest: 0.7868661\tbest: 0.7868661 (605)\ttotal: 2m 44s\tremaining: 1m 47s\n",
            "606:\ttotal: 2m 44s\tremaining: 1m 46s\n",
            "607:\ttotal: 2m 45s\tremaining: 1m 46s\n",
            "608:\ttotal: 2m 45s\tremaining: 1m 46s\n",
            "609:\ttotal: 2m 45s\tremaining: 1m 45s\n",
            "610:\ttest: 0.7869090\tbest: 0.7869090 (610)\ttotal: 2m 46s\tremaining: 1m 45s\n",
            "611:\ttotal: 2m 46s\tremaining: 1m 45s\n",
            "612:\ttotal: 2m 46s\tremaining: 1m 45s\n",
            "613:\ttotal: 2m 46s\tremaining: 1m 44s\n",
            "614:\ttotal: 2m 47s\tremaining: 1m 44s\n",
            "615:\ttest: 0.7869495\tbest: 0.7869495 (615)\ttotal: 2m 47s\tremaining: 1m 44s\n",
            "616:\ttotal: 2m 47s\tremaining: 1m 44s\n",
            "617:\ttotal: 2m 48s\tremaining: 1m 43s\n",
            "618:\ttotal: 2m 48s\tremaining: 1m 43s\n",
            "619:\ttotal: 2m 48s\tremaining: 1m 43s\n",
            "620:\ttest: 0.7870069\tbest: 0.7870069 (620)\ttotal: 2m 48s\tremaining: 1m 43s\n",
            "621:\ttotal: 2m 49s\tremaining: 1m 42s\n",
            "622:\ttotal: 2m 49s\tremaining: 1m 42s\n",
            "623:\ttotal: 2m 49s\tremaining: 1m 42s\n",
            "624:\ttotal: 2m 49s\tremaining: 1m 41s\n",
            "625:\ttest: 0.7870633\tbest: 0.7870633 (625)\ttotal: 2m 50s\tremaining: 1m 41s\n",
            "626:\ttotal: 2m 50s\tremaining: 1m 41s\n",
            "627:\ttotal: 2m 50s\tremaining: 1m 41s\n",
            "628:\ttotal: 2m 50s\tremaining: 1m 40s\n",
            "629:\ttotal: 2m 51s\tremaining: 1m 40s\n",
            "630:\ttest: 0.7871241\tbest: 0.7871241 (630)\ttotal: 2m 51s\tremaining: 1m 40s\n",
            "631:\ttotal: 2m 51s\tremaining: 1m 39s\n",
            "632:\ttotal: 2m 51s\tremaining: 1m 39s\n",
            "633:\ttotal: 2m 51s\tremaining: 1m 39s\n",
            "634:\ttotal: 2m 52s\tremaining: 1m 38s\n",
            "635:\ttest: 0.7871969\tbest: 0.7871969 (635)\ttotal: 2m 52s\tremaining: 1m 38s\n",
            "636:\ttotal: 2m 52s\tremaining: 1m 38s\n",
            "637:\ttotal: 2m 52s\tremaining: 1m 38s\n",
            "638:\ttotal: 2m 53s\tremaining: 1m 37s\n",
            "639:\ttotal: 2m 53s\tremaining: 1m 37s\n",
            "640:\ttest: 0.7872517\tbest: 0.7872517 (640)\ttotal: 2m 53s\tremaining: 1m 37s\n",
            "641:\ttotal: 2m 53s\tremaining: 1m 36s\n",
            "642:\ttotal: 2m 54s\tremaining: 1m 36s\n",
            "643:\ttotal: 2m 54s\tremaining: 1m 36s\n",
            "644:\ttotal: 2m 54s\tremaining: 1m 36s\n",
            "645:\ttest: 0.7873096\tbest: 0.7873096 (645)\ttotal: 2m 55s\tremaining: 1m 35s\n",
            "646:\ttotal: 2m 55s\tremaining: 1m 35s\n",
            "647:\ttotal: 2m 55s\tremaining: 1m 35s\n",
            "648:\ttotal: 2m 55s\tremaining: 1m 35s\n",
            "649:\ttotal: 2m 56s\tremaining: 1m 34s\n",
            "650:\ttest: 0.7873581\tbest: 0.7873581 (650)\ttotal: 2m 56s\tremaining: 1m 34s\n",
            "651:\ttotal: 2m 56s\tremaining: 1m 34s\n",
            "652:\ttotal: 2m 56s\tremaining: 1m 33s\n",
            "653:\ttotal: 2m 57s\tremaining: 1m 33s\n",
            "654:\ttotal: 2m 57s\tremaining: 1m 33s\n",
            "655:\ttest: 0.7874150\tbest: 0.7874150 (655)\ttotal: 2m 57s\tremaining: 1m 33s\n",
            "656:\ttotal: 2m 58s\tremaining: 1m 33s\n",
            "657:\ttotal: 2m 58s\tremaining: 1m 32s\n",
            "658:\ttotal: 2m 58s\tremaining: 1m 32s\n",
            "659:\ttotal: 2m 59s\tremaining: 1m 32s\n",
            "660:\ttest: 0.7874601\tbest: 0.7874601 (660)\ttotal: 2m 59s\tremaining: 1m 31s\n",
            "661:\ttotal: 2m 59s\tremaining: 1m 31s\n",
            "662:\ttotal: 2m 59s\tremaining: 1m 31s\n",
            "663:\ttotal: 2m 59s\tremaining: 1m 31s\n",
            "664:\ttotal: 3m\tremaining: 1m 30s\n",
            "665:\ttest: 0.7875333\tbest: 0.7875333 (665)\ttotal: 3m\tremaining: 1m 30s\n",
            "666:\ttotal: 3m\tremaining: 1m 30s\n",
            "667:\ttotal: 3m\tremaining: 1m 29s\n",
            "668:\ttotal: 3m 1s\tremaining: 1m 29s\n",
            "669:\ttotal: 3m 1s\tremaining: 1m 29s\n",
            "670:\ttest: 0.7875690\tbest: 0.7875690 (670)\ttotal: 3m 1s\tremaining: 1m 29s\n",
            "671:\ttotal: 3m 2s\tremaining: 1m 28s\n",
            "672:\ttotal: 3m 2s\tremaining: 1m 28s\n",
            "673:\ttotal: 3m 2s\tremaining: 1m 28s\n",
            "674:\ttotal: 3m 3s\tremaining: 1m 28s\n",
            "675:\ttest: 0.7876154\tbest: 0.7876154 (675)\ttotal: 3m 3s\tremaining: 1m 27s\n",
            "676:\ttotal: 3m 3s\tremaining: 1m 27s\n",
            "677:\ttotal: 3m 4s\tremaining: 1m 27s\n",
            "678:\ttotal: 3m 4s\tremaining: 1m 27s\n",
            "679:\ttotal: 3m 4s\tremaining: 1m 26s\n",
            "680:\ttest: 0.7876812\tbest: 0.7876812 (680)\ttotal: 3m 5s\tremaining: 1m 26s\n",
            "681:\ttotal: 3m 5s\tremaining: 1m 26s\n",
            "682:\ttotal: 3m 5s\tremaining: 1m 26s\n",
            "683:\ttotal: 3m 5s\tremaining: 1m 25s\n",
            "684:\ttotal: 3m 5s\tremaining: 1m 25s\n",
            "685:\ttest: 0.7877368\tbest: 0.7877368 (685)\ttotal: 3m 6s\tremaining: 1m 25s\n",
            "686:\ttotal: 3m 6s\tremaining: 1m 25s\n",
            "687:\ttotal: 3m 6s\tremaining: 1m 24s\n",
            "688:\ttotal: 3m 7s\tremaining: 1m 24s\n",
            "689:\ttotal: 3m 7s\tremaining: 1m 24s\n",
            "690:\ttest: 0.7877861\tbest: 0.7877861 (690)\ttotal: 3m 7s\tremaining: 1m 23s\n",
            "691:\ttotal: 3m 7s\tremaining: 1m 23s\n",
            "692:\ttotal: 3m 7s\tremaining: 1m 23s\n",
            "693:\ttotal: 3m 7s\tremaining: 1m 22s\n",
            "694:\ttotal: 3m 8s\tremaining: 1m 22s\n",
            "695:\ttest: 0.7878287\tbest: 0.7878287 (695)\ttotal: 3m 8s\tremaining: 1m 22s\n",
            "696:\ttotal: 3m 8s\tremaining: 1m 22s\n",
            "697:\ttotal: 3m 9s\tremaining: 1m 21s\n",
            "698:\ttotal: 3m 9s\tremaining: 1m 21s\n",
            "699:\ttotal: 3m 9s\tremaining: 1m 21s\n",
            "700:\ttest: 0.7878823\tbest: 0.7878823 (700)\ttotal: 3m 9s\tremaining: 1m 21s\n",
            "701:\ttotal: 3m 10s\tremaining: 1m 20s\n",
            "702:\ttotal: 3m 10s\tremaining: 1m 20s\n",
            "703:\ttotal: 3m 10s\tremaining: 1m 20s\n",
            "704:\ttotal: 3m 10s\tremaining: 1m 19s\n",
            "705:\ttest: 0.7879313\tbest: 0.7879313 (705)\ttotal: 3m 11s\tremaining: 1m 19s\n",
            "706:\ttotal: 3m 11s\tremaining: 1m 19s\n",
            "707:\ttotal: 3m 12s\tremaining: 1m 19s\n",
            "708:\ttotal: 3m 12s\tremaining: 1m 18s\n",
            "709:\ttotal: 3m 12s\tremaining: 1m 18s\n",
            "710:\ttest: 0.7879965\tbest: 0.7879965 (710)\ttotal: 3m 12s\tremaining: 1m 18s\n",
            "711:\ttotal: 3m 12s\tremaining: 1m 18s\n",
            "712:\ttotal: 3m 13s\tremaining: 1m 17s\n",
            "713:\ttotal: 3m 13s\tremaining: 1m 17s\n",
            "714:\ttotal: 3m 14s\tremaining: 1m 17s\n",
            "715:\ttest: 0.7880360\tbest: 0.7880360 (715)\ttotal: 3m 15s\tremaining: 1m 17s\n",
            "716:\ttotal: 3m 15s\tremaining: 1m 17s\n",
            "717:\ttotal: 3m 15s\tremaining: 1m 16s\n",
            "718:\ttotal: 3m 16s\tremaining: 1m 16s\n",
            "719:\ttotal: 3m 16s\tremaining: 1m 16s\n",
            "720:\ttest: 0.7880856\tbest: 0.7880856 (720)\ttotal: 3m 16s\tremaining: 1m 16s\n",
            "721:\ttotal: 3m 17s\tremaining: 1m 15s\n",
            "722:\ttotal: 3m 17s\tremaining: 1m 15s\n",
            "723:\ttotal: 3m 17s\tremaining: 1m 15s\n",
            "724:\ttotal: 3m 17s\tremaining: 1m 15s\n",
            "725:\ttest: 0.7881790\tbest: 0.7881790 (725)\ttotal: 3m 18s\tremaining: 1m 14s\n",
            "726:\ttotal: 3m 18s\tremaining: 1m 14s\n",
            "727:\ttotal: 3m 18s\tremaining: 1m 14s\n",
            "728:\ttotal: 3m 19s\tremaining: 1m 14s\n",
            "729:\ttotal: 3m 19s\tremaining: 1m 13s\n",
            "730:\ttest: 0.7882334\tbest: 0.7882334 (730)\ttotal: 3m 19s\tremaining: 1m 13s\n",
            "731:\ttotal: 3m 19s\tremaining: 1m 13s\n",
            "732:\ttotal: 3m 20s\tremaining: 1m 12s\n",
            "733:\ttotal: 3m 20s\tremaining: 1m 12s\n",
            "734:\ttotal: 3m 20s\tremaining: 1m 12s\n",
            "735:\ttest: 0.7883014\tbest: 0.7883014 (735)\ttotal: 3m 20s\tremaining: 1m 11s\n",
            "736:\ttotal: 3m 21s\tremaining: 1m 11s\n",
            "737:\ttotal: 3m 21s\tremaining: 1m 11s\n",
            "738:\ttotal: 3m 21s\tremaining: 1m 11s\n",
            "739:\ttotal: 3m 21s\tremaining: 1m 10s\n",
            "740:\ttest: 0.7883464\tbest: 0.7883464 (740)\ttotal: 3m 22s\tremaining: 1m 10s\n",
            "741:\ttotal: 3m 22s\tremaining: 1m 10s\n",
            "742:\ttotal: 3m 22s\tremaining: 1m 10s\n",
            "743:\ttotal: 3m 22s\tremaining: 1m 9s\n",
            "744:\ttotal: 3m 23s\tremaining: 1m 9s\n",
            "745:\ttest: 0.7884136\tbest: 0.7884136 (745)\ttotal: 3m 23s\tremaining: 1m 9s\n",
            "746:\ttotal: 3m 23s\tremaining: 1m 8s\n",
            "747:\ttotal: 3m 23s\tremaining: 1m 8s\n",
            "748:\ttotal: 3m 23s\tremaining: 1m 8s\n",
            "749:\ttotal: 3m 24s\tremaining: 1m 8s\n",
            "750:\ttest: 0.7884707\tbest: 0.7884707 (750)\ttotal: 3m 24s\tremaining: 1m 7s\n",
            "751:\ttotal: 3m 24s\tremaining: 1m 7s\n",
            "752:\ttotal: 3m 24s\tremaining: 1m 7s\n",
            "753:\ttotal: 3m 24s\tremaining: 1m 6s\n",
            "754:\ttotal: 3m 25s\tremaining: 1m 6s\n",
            "755:\ttest: 0.7885256\tbest: 0.7885256 (755)\ttotal: 3m 25s\tremaining: 1m 6s\n",
            "756:\ttotal: 3m 25s\tremaining: 1m 6s\n",
            "757:\ttotal: 3m 26s\tremaining: 1m 5s\n",
            "758:\ttotal: 3m 26s\tremaining: 1m 5s\n",
            "759:\ttotal: 3m 26s\tremaining: 1m 5s\n",
            "760:\ttest: 0.7885658\tbest: 0.7885658 (760)\ttotal: 3m 26s\tremaining: 1m 4s\n",
            "761:\ttotal: 3m 26s\tremaining: 1m 4s\n",
            "762:\ttotal: 3m 26s\tremaining: 1m 4s\n",
            "763:\ttotal: 3m 27s\tremaining: 1m 4s\n",
            "764:\ttotal: 3m 27s\tremaining: 1m 3s\n",
            "765:\ttest: 0.7885991\tbest: 0.7885991 (765)\ttotal: 3m 27s\tremaining: 1m 3s\n",
            "766:\ttotal: 3m 28s\tremaining: 1m 3s\n",
            "767:\ttotal: 3m 28s\tremaining: 1m 3s\n",
            "768:\ttotal: 3m 29s\tremaining: 1m 2s\n",
            "769:\ttotal: 3m 29s\tremaining: 1m 2s\n",
            "770:\ttest: 0.7886606\tbest: 0.7886606 (770)\ttotal: 3m 29s\tremaining: 1m 2s\n",
            "771:\ttotal: 3m 29s\tremaining: 1m 1s\n",
            "772:\ttotal: 3m 30s\tremaining: 1m 1s\n",
            "773:\ttotal: 3m 30s\tremaining: 1m 1s\n",
            "774:\ttotal: 3m 30s\tremaining: 1m 1s\n",
            "775:\ttest: 0.7887204\tbest: 0.7887204 (775)\ttotal: 3m 30s\tremaining: 1m\n",
            "776:\ttotal: 3m 30s\tremaining: 1m\n",
            "777:\ttotal: 3m 31s\tremaining: 1m\n",
            "778:\ttotal: 3m 31s\tremaining: 59.9s\n",
            "779:\ttotal: 3m 31s\tremaining: 59.6s\n",
            "780:\ttest: 0.7887561\tbest: 0.7887561 (780)\ttotal: 3m 31s\tremaining: 59.4s\n",
            "781:\ttotal: 3m 31s\tremaining: 59.1s\n",
            "782:\ttotal: 3m 32s\tremaining: 58.8s\n",
            "783:\ttotal: 3m 32s\tremaining: 58.5s\n",
            "784:\ttotal: 3m 32s\tremaining: 58.2s\n",
            "785:\ttest: 0.7887937\tbest: 0.7887937 (785)\ttotal: 3m 32s\tremaining: 57.9s\n",
            "786:\ttotal: 3m 32s\tremaining: 57.6s\n",
            "787:\ttotal: 3m 33s\tremaining: 57.3s\n",
            "788:\ttotal: 3m 33s\tremaining: 57.1s\n",
            "789:\ttotal: 3m 33s\tremaining: 56.8s\n",
            "790:\ttest: 0.7888297\tbest: 0.7888297 (790)\ttotal: 3m 33s\tremaining: 56.5s\n",
            "791:\ttotal: 3m 34s\tremaining: 56.3s\n",
            "792:\ttotal: 3m 34s\tremaining: 56s\n",
            "793:\ttotal: 3m 34s\tremaining: 55.7s\n",
            "794:\ttotal: 3m 35s\tremaining: 55.5s\n",
            "795:\ttest: 0.7888783\tbest: 0.7888783 (795)\ttotal: 3m 35s\tremaining: 55.2s\n",
            "796:\ttotal: 3m 35s\tremaining: 54.9s\n",
            "797:\ttotal: 3m 35s\tremaining: 54.6s\n",
            "798:\ttotal: 3m 35s\tremaining: 54.3s\n",
            "799:\ttotal: 3m 36s\tremaining: 54s\n",
            "800:\ttest: 0.7889465\tbest: 0.7889465 (800)\ttotal: 3m 36s\tremaining: 53.8s\n",
            "801:\ttotal: 3m 36s\tremaining: 53.5s\n",
            "802:\ttotal: 3m 37s\tremaining: 53.2s\n",
            "803:\ttotal: 3m 37s\tremaining: 53s\n",
            "804:\ttotal: 3m 37s\tremaining: 52.7s\n",
            "805:\ttest: 0.7889777\tbest: 0.7889777 (805)\ttotal: 3m 37s\tremaining: 52.5s\n",
            "806:\ttotal: 3m 38s\tremaining: 52.2s\n",
            "807:\ttotal: 3m 38s\tremaining: 51.9s\n",
            "808:\ttotal: 3m 38s\tremaining: 51.6s\n",
            "809:\ttotal: 3m 38s\tremaining: 51.4s\n",
            "810:\ttest: 0.7890337\tbest: 0.7890337 (810)\ttotal: 3m 39s\tremaining: 51.1s\n",
            "811:\ttotal: 3m 39s\tremaining: 50.8s\n",
            "812:\ttotal: 3m 39s\tremaining: 50.5s\n",
            "813:\ttotal: 3m 39s\tremaining: 50.2s\n",
            "814:\ttotal: 3m 39s\tremaining: 49.9s\n",
            "815:\ttest: 0.7890856\tbest: 0.7890856 (815)\ttotal: 3m 40s\tremaining: 49.7s\n",
            "816:\ttotal: 3m 40s\tremaining: 49.4s\n",
            "817:\ttotal: 3m 40s\tremaining: 49.1s\n",
            "818:\ttotal: 3m 40s\tremaining: 48.8s\n",
            "819:\ttotal: 3m 41s\tremaining: 48.5s\n",
            "820:\ttest: 0.7891319\tbest: 0.7891319 (820)\ttotal: 3m 41s\tremaining: 48.2s\n",
            "821:\ttotal: 3m 41s\tremaining: 48s\n",
            "822:\ttotal: 3m 41s\tremaining: 47.7s\n",
            "823:\ttotal: 3m 41s\tremaining: 47.4s\n",
            "824:\ttotal: 3m 42s\tremaining: 47.1s\n",
            "825:\ttest: 0.7891780\tbest: 0.7891780 (825)\ttotal: 3m 42s\tremaining: 46.8s\n",
            "826:\ttotal: 3m 42s\tremaining: 46.5s\n",
            "827:\ttotal: 3m 42s\tremaining: 46.3s\n",
            "828:\ttotal: 3m 42s\tremaining: 46s\n",
            "829:\ttotal: 3m 43s\tremaining: 45.7s\n",
            "830:\ttest: 0.7892153\tbest: 0.7892153 (830)\ttotal: 3m 43s\tremaining: 45.4s\n",
            "831:\ttotal: 3m 43s\tremaining: 45.1s\n",
            "832:\ttotal: 3m 43s\tremaining: 44.9s\n",
            "833:\ttotal: 3m 43s\tremaining: 44.6s\n",
            "834:\ttotal: 3m 44s\tremaining: 44.3s\n",
            "835:\ttest: 0.7892723\tbest: 0.7892723 (835)\ttotal: 3m 44s\tremaining: 44.1s\n",
            "836:\ttotal: 3m 45s\tremaining: 43.8s\n",
            "837:\ttotal: 3m 45s\tremaining: 43.6s\n",
            "838:\ttotal: 3m 45s\tremaining: 43.3s\n",
            "839:\ttotal: 3m 45s\tremaining: 43s\n",
            "840:\ttest: 0.7893210\tbest: 0.7893210 (840)\ttotal: 3m 46s\tremaining: 42.8s\n",
            "841:\ttotal: 3m 46s\tremaining: 42.5s\n",
            "842:\ttotal: 3m 46s\tremaining: 42.2s\n",
            "843:\ttotal: 3m 46s\tremaining: 41.9s\n",
            "844:\ttotal: 3m 47s\tremaining: 41.7s\n",
            "845:\ttest: 0.7893525\tbest: 0.7893525 (845)\ttotal: 3m 47s\tremaining: 41.4s\n",
            "846:\ttotal: 3m 47s\tremaining: 41.1s\n",
            "847:\ttotal: 3m 47s\tremaining: 40.8s\n",
            "848:\ttotal: 3m 47s\tremaining: 40.5s\n",
            "849:\ttotal: 3m 48s\tremaining: 40.3s\n",
            "850:\ttest: 0.7893974\tbest: 0.7893974 (850)\ttotal: 3m 48s\tremaining: 40s\n",
            "851:\ttotal: 3m 49s\tremaining: 39.8s\n",
            "852:\ttotal: 3m 49s\tremaining: 39.5s\n",
            "853:\ttotal: 3m 49s\tremaining: 39.2s\n",
            "854:\ttotal: 3m 49s\tremaining: 38.9s\n",
            "855:\ttest: 0.7894494\tbest: 0.7894494 (855)\ttotal: 3m 49s\tremaining: 38.7s\n",
            "856:\ttotal: 3m 50s\tremaining: 38.4s\n",
            "857:\ttotal: 3m 50s\tremaining: 38.1s\n",
            "858:\ttotal: 3m 50s\tremaining: 37.9s\n",
            "859:\ttotal: 3m 50s\tremaining: 37.6s\n",
            "860:\ttest: 0.7894827\tbest: 0.7894827 (860)\ttotal: 3m 51s\tremaining: 37.4s\n",
            "861:\ttotal: 3m 52s\tremaining: 37.2s\n",
            "862:\ttotal: 3m 52s\tremaining: 36.9s\n",
            "863:\ttotal: 3m 52s\tremaining: 36.7s\n",
            "864:\ttotal: 3m 53s\tremaining: 36.4s\n",
            "865:\ttest: 0.7895317\tbest: 0.7895317 (865)\ttotal: 3m 53s\tremaining: 36.1s\n",
            "866:\ttotal: 3m 53s\tremaining: 35.9s\n",
            "867:\ttotal: 3m 54s\tremaining: 35.6s\n",
            "868:\ttotal: 3m 54s\tremaining: 35.4s\n",
            "869:\ttotal: 3m 54s\tremaining: 35.1s\n",
            "870:\ttest: 0.7895668\tbest: 0.7895668 (870)\ttotal: 3m 55s\tremaining: 34.8s\n",
            "871:\ttotal: 3m 55s\tremaining: 34.6s\n",
            "872:\ttotal: 3m 55s\tremaining: 34.3s\n",
            "873:\ttotal: 3m 56s\tremaining: 34.1s\n",
            "874:\ttotal: 3m 56s\tremaining: 33.8s\n",
            "875:\ttest: 0.7896020\tbest: 0.7896020 (875)\ttotal: 3m 56s\tremaining: 33.5s\n",
            "876:\ttotal: 3m 57s\tremaining: 33.3s\n",
            "877:\ttotal: 3m 57s\tremaining: 33s\n",
            "878:\ttotal: 3m 57s\tremaining: 32.7s\n",
            "879:\ttotal: 3m 58s\tremaining: 32.5s\n",
            "880:\ttest: 0.7896329\tbest: 0.7896329 (880)\ttotal: 3m 58s\tremaining: 32.2s\n",
            "881:\ttotal: 3m 58s\tremaining: 31.9s\n",
            "882:\ttotal: 3m 58s\tremaining: 31.6s\n",
            "883:\ttotal: 3m 59s\tremaining: 31.4s\n",
            "884:\ttotal: 3m 59s\tremaining: 31.1s\n",
            "885:\ttest: 0.7896608\tbest: 0.7896608 (885)\ttotal: 3m 59s\tremaining: 30.8s\n",
            "886:\ttotal: 4m\tremaining: 30.6s\n",
            "887:\ttotal: 4m\tremaining: 30.3s\n",
            "888:\ttotal: 4m\tremaining: 30s\n",
            "889:\ttotal: 4m\tremaining: 29.8s\n",
            "890:\ttest: 0.7897059\tbest: 0.7897059 (890)\ttotal: 4m 1s\tremaining: 29.5s\n",
            "891:\ttotal: 4m 1s\tremaining: 29.3s\n",
            "892:\ttotal: 4m 2s\tremaining: 29s\n",
            "893:\ttotal: 4m 2s\tremaining: 28.7s\n",
            "894:\ttotal: 4m 2s\tremaining: 28.5s\n",
            "895:\ttest: 0.7897298\tbest: 0.7897298 (895)\ttotal: 4m 3s\tremaining: 28.2s\n",
            "896:\ttotal: 4m 3s\tremaining: 28s\n",
            "897:\ttotal: 4m 3s\tremaining: 27.7s\n",
            "898:\ttotal: 4m 3s\tremaining: 27.4s\n",
            "899:\ttotal: 4m 4s\tremaining: 27.1s\n",
            "900:\ttest: 0.7897587\tbest: 0.7897587 (900)\ttotal: 4m 4s\tremaining: 26.8s\n",
            "901:\ttotal: 4m 4s\tremaining: 26.6s\n",
            "902:\ttotal: 4m 5s\tremaining: 26.3s\n",
            "903:\ttotal: 4m 5s\tremaining: 26s\n",
            "904:\ttotal: 4m 5s\tremaining: 25.8s\n",
            "905:\ttest: 0.7898046\tbest: 0.7898046 (905)\ttotal: 4m 5s\tremaining: 25.5s\n",
            "906:\ttotal: 4m 6s\tremaining: 25.3s\n",
            "907:\ttotal: 4m 6s\tremaining: 25s\n",
            "908:\ttotal: 4m 6s\tremaining: 24.7s\n",
            "909:\ttotal: 4m 7s\tremaining: 24.5s\n",
            "910:\ttest: 0.7898487\tbest: 0.7898487 (910)\ttotal: 4m 7s\tremaining: 24.2s\n",
            "911:\ttotal: 4m 8s\tremaining: 24s\n",
            "912:\ttotal: 4m 8s\tremaining: 23.7s\n",
            "913:\ttotal: 4m 8s\tremaining: 23.4s\n",
            "914:\ttotal: 4m 9s\tremaining: 23.1s\n",
            "915:\ttest: 0.7898852\tbest: 0.7898852 (915)\ttotal: 4m 9s\tremaining: 22.9s\n",
            "916:\ttotal: 4m 10s\tremaining: 22.6s\n",
            "917:\ttotal: 4m 10s\tremaining: 22.4s\n",
            "918:\ttotal: 4m 10s\tremaining: 22.1s\n",
            "919:\ttotal: 4m 10s\tremaining: 21.8s\n",
            "920:\ttest: 0.7899287\tbest: 0.7899287 (920)\ttotal: 4m 11s\tremaining: 21.5s\n",
            "921:\ttotal: 4m 11s\tremaining: 21.3s\n",
            "922:\ttotal: 4m 11s\tremaining: 21s\n",
            "923:\ttotal: 4m 11s\tremaining: 20.7s\n",
            "924:\ttotal: 4m 11s\tremaining: 20.4s\n",
            "925:\ttest: 0.7899627\tbest: 0.7899627 (925)\ttotal: 4m 12s\tremaining: 20.1s\n",
            "926:\ttotal: 4m 12s\tremaining: 19.9s\n",
            "927:\ttotal: 4m 12s\tremaining: 19.6s\n",
            "928:\ttotal: 4m 13s\tremaining: 19.3s\n",
            "929:\ttotal: 4m 13s\tremaining: 19.1s\n",
            "930:\ttest: 0.7899882\tbest: 0.7899882 (930)\ttotal: 4m 13s\tremaining: 18.8s\n",
            "931:\ttotal: 4m 14s\tremaining: 18.5s\n",
            "932:\ttotal: 4m 14s\tremaining: 18.3s\n",
            "933:\ttotal: 4m 15s\tremaining: 18s\n",
            "934:\ttotal: 4m 15s\tremaining: 17.7s\n",
            "935:\ttest: 0.7900208\tbest: 0.7900208 (935)\ttotal: 4m 15s\tremaining: 17.5s\n",
            "936:\ttotal: 4m 15s\tremaining: 17.2s\n",
            "937:\ttotal: 4m 16s\tremaining: 16.9s\n",
            "938:\ttotal: 4m 16s\tremaining: 16.7s\n",
            "939:\ttotal: 4m 16s\tremaining: 16.4s\n",
            "940:\ttest: 0.7900591\tbest: 0.7900591 (940)\ttotal: 4m 17s\tremaining: 16.1s\n",
            "941:\ttotal: 4m 17s\tremaining: 15.9s\n",
            "942:\ttotal: 4m 18s\tremaining: 15.6s\n",
            "943:\ttotal: 4m 18s\tremaining: 15.3s\n",
            "944:\ttotal: 4m 18s\tremaining: 15.1s\n",
            "945:\ttest: 0.7900947\tbest: 0.7900947 (945)\ttotal: 4m 19s\tremaining: 14.8s\n",
            "946:\ttotal: 4m 19s\tremaining: 14.5s\n",
            "947:\ttotal: 4m 19s\tremaining: 14.2s\n",
            "948:\ttotal: 4m 19s\tremaining: 14s\n",
            "949:\ttotal: 4m 20s\tremaining: 13.7s\n",
            "950:\ttest: 0.7901275\tbest: 0.7901275 (950)\ttotal: 4m 20s\tremaining: 13.4s\n",
            "951:\ttotal: 4m 21s\tremaining: 13.2s\n",
            "952:\ttotal: 4m 21s\tremaining: 12.9s\n",
            "953:\ttotal: 4m 22s\tremaining: 12.6s\n",
            "954:\ttotal: 4m 22s\tremaining: 12.4s\n",
            "955:\ttest: 0.7901551\tbest: 0.7901551 (955)\ttotal: 4m 22s\tremaining: 12.1s\n",
            "956:\ttotal: 4m 22s\tremaining: 11.8s\n",
            "957:\ttotal: 4m 23s\tremaining: 11.5s\n",
            "958:\ttotal: 4m 23s\tremaining: 11.3s\n",
            "959:\ttotal: 4m 23s\tremaining: 11s\n",
            "960:\ttest: 0.7902078\tbest: 0.7902078 (960)\ttotal: 4m 23s\tremaining: 10.7s\n",
            "961:\ttotal: 4m 24s\tremaining: 10.4s\n",
            "962:\ttotal: 4m 24s\tremaining: 10.2s\n",
            "963:\ttotal: 4m 24s\tremaining: 9.88s\n",
            "964:\ttotal: 4m 24s\tremaining: 9.6s\n",
            "965:\ttest: 0.7902319\tbest: 0.7902319 (965)\ttotal: 4m 24s\tremaining: 9.32s\n",
            "966:\ttotal: 4m 25s\tremaining: 9.06s\n",
            "967:\ttotal: 4m 25s\tremaining: 8.78s\n",
            "968:\ttotal: 4m 25s\tremaining: 8.5s\n",
            "969:\ttotal: 4m 25s\tremaining: 8.23s\n",
            "970:\ttest: 0.7902803\tbest: 0.7902803 (970)\ttotal: 4m 26s\tremaining: 7.95s\n",
            "971:\ttotal: 4m 26s\tremaining: 7.67s\n",
            "972:\ttotal: 4m 26s\tremaining: 7.41s\n",
            "973:\ttotal: 4m 27s\tremaining: 7.13s\n",
            "974:\ttotal: 4m 27s\tremaining: 6.85s\n",
            "975:\ttest: 0.7903121\tbest: 0.7903121 (975)\ttotal: 4m 27s\tremaining: 6.58s\n",
            "976:\ttotal: 4m 27s\tremaining: 6.3s\n",
            "977:\ttotal: 4m 28s\tremaining: 6.04s\n",
            "978:\ttotal: 4m 28s\tremaining: 5.76s\n",
            "979:\ttotal: 4m 28s\tremaining: 5.49s\n",
            "980:\ttest: 0.7903438\tbest: 0.7903438 (980)\ttotal: 4m 29s\tremaining: 5.21s\n",
            "981:\ttotal: 4m 29s\tremaining: 4.93s\n",
            "982:\ttotal: 4m 29s\tremaining: 4.66s\n",
            "983:\ttotal: 4m 29s\tremaining: 4.38s\n",
            "984:\ttotal: 4m 30s\tremaining: 4.11s\n",
            "985:\ttest: 0.7903702\tbest: 0.7903702 (985)\ttotal: 4m 30s\tremaining: 3.84s\n",
            "986:\ttotal: 4m 30s\tremaining: 3.56s\n",
            "987:\ttotal: 4m 30s\tremaining: 3.29s\n",
            "988:\ttotal: 4m 30s\tremaining: 3.01s\n",
            "989:\ttotal: 4m 31s\tremaining: 2.74s\n",
            "990:\ttest: 0.7903959\tbest: 0.7903959 (990)\ttotal: 4m 31s\tremaining: 2.46s\n",
            "991:\ttotal: 4m 31s\tremaining: 2.19s\n",
            "992:\ttotal: 4m 31s\tremaining: 1.91s\n",
            "993:\ttotal: 4m 31s\tremaining: 1.64s\n",
            "994:\ttotal: 4m 32s\tremaining: 1.37s\n",
            "995:\ttest: 0.7904420\tbest: 0.7904420 (995)\ttotal: 4m 32s\tremaining: 1.09s\n",
            "996:\ttotal: 4m 32s\tremaining: 820ms\n",
            "997:\ttotal: 4m 32s\tremaining: 546ms\n",
            "998:\ttotal: 4m 32s\tremaining: 273ms\n",
            "999:\ttest: 0.7904579\tbest: 0.7904579 (999)\ttotal: 4m 33s\tremaining: 0us\n",
            "bestTest = 0.7904579341\n",
            "bestIteration = 999\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x7efb19a02780>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import catboost as cb\n",
        "\n",
        "X_train = train_df.drop('label').to_pandas()\n",
        "y_train = train_df['label'].to_pandas()\n",
        "X_test = test_df.drop('label').to_pandas()\n",
        "y_test = test_df['label'].to_pandas()\n",
        "\n",
        "train_pool = cb.Pool(X_train, y_train, cat_features=CriteoDatasetUtils.CAT_COLS)\n",
        "test_pool = cb.Pool(X_test, y_test, cat_features=CriteoDatasetUtils.CAT_COLS)\n",
        "\n",
        "model = cb.CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    loss_function=\"Logloss\",\n",
        "    eval_metric=\"AUC\",\n",
        "    early_stopping_rounds=50,\n",
        "    task_type=\"GPU\",\n",
        "    devices='1'   \n",
        ")\n",
        "model.fit(train_pool, eval_set=test_pool, use_best_model=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Из [DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems](https://arxiv.org/abs/2008.13535):\n",
        "<div style=\"width:50%; margin: auto;\">\n",
        "\n",
        "![](https://i.ibb.co/HDHJ8Nzq/level-improvement.png)\n",
        "![](https://i.ibb.co/fYpyrKBs/table.png)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Применяем полученные знания в YSDA-RecSys-2025 Lavka\n",
        "\n",
        "- Заменяем FFN на ResNet или DenseNet с лекции.\n",
        "- Перебираем гиперпараметры (размер модели, batch_size, lr, ...).\n",
        "- Ускоряем обучения за счет DCN-Mix (см. [DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems](https://arxiv.org/abs/2008.13535))."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
